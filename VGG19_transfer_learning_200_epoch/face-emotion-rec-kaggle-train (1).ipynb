{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"! pip install imutils","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting imutils\n  Downloading imutils-0.5.4.tar.gz (17 kB)\nBuilding wheels for collected packages: imutils\n  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25858 sha256=1d961ec32afd98989e657d1cbde5fd9fea1d55d632c5a33c8c6c15c4ab9d8610\n  Stored in directory: /root/.cache/pip/wheels/86/d7/0a/4923351ed1cec5d5e24c1eaf8905567b02a0343b24aa873df2\nSuccessfully built imutils\nInstalling collected packages: imutils\nSuccessfully installed imutils-0.5.4\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\n# from tensorflow.keras.models import load_model\n# from keras.layers import LeakyReLU\nfrom sklearn.preprocessing import LabelBinarizer,LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport os","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[INFO] loading images...\")\nimagePaths = list(paths.list_images(\"../input/human-anime/human_anime\"))\n# print(imagePaths)\ndata = []\nlabels = []","execution_count":12,"outputs":[{"output_type":"stream","text":"[INFO] loading images...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for imagePath in imagePaths:\n\t# extract the class label from the filename\n\tlabel = imagePath.split(os.path.sep)[-2]\n\n\t# load the input image (224x224) and preprocess it\n\timage = load_img(imagePath, target_size=(72, 72))\n\timage = img_to_array(image)\n# \timage = preprocess_input(image)\n\t# update the data and labels lists, respectively\n\t# count+=1/\n\t# if count%10==0:\n\tdata.append(image)\n\tlabels.append(label)\n    \n","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.array(data, dtype=\"float32\")\nlabels1 = np.array(labels)\ndel labels\nprint(\"labels\",labels1.shape)\n# perform one-hot encoding on the labels\nlb = LabelEncoder()\nlabels = lb.fit_transform(labels1)\nlabels = to_categorical(labels)\nprint(labels,type(labels))","execution_count":14,"outputs":[{"output_type":"stream","text":"labels (37369,)\n[[0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 0. 1.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]] <class 'numpy.ndarray'>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape,labels.shape)\n(trainX, testX, trainY, testY) = train_test_split(data, labels,\n\ttest_size=0.2, stratify=labels, random_state=42)\nprint(trainX.shape, testX.shape, trainY.shape, testY.shape)","execution_count":15,"outputs":[{"output_type":"stream","text":"(37369, 72, 72, 3) (37369, 7)\n(29895, 72, 72, 3) (7474, 72, 72, 3) (29895, 7) (7474, 7)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = ImageDataGenerator(\n\trotation_range=20,\n\tzoom_range=0.15,\n\twidth_shift_range=0.2,\n\theight_shift_range=0.2,\n\tshear_range=0.15,\n\thorizontal_flip=True,\n\tfill_mode=\"nearest\")","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import InputLayer,Conv2D,MaxPool2D,BatchNormalization\nfrom keras import regularizers\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.models import load_model","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INIT_LR = 1e-3\nEPOCHS = 200\nBS = 256\n\n\nbaseModel = Sequential()\n# baseModel.add(Dense(12, activation='relu', input_tensor = Input(shape=(48,48,3))))\n# baseModel.add(InputLayer(input_shape = (48,48,3)))\nbaseModel = tf.keras.applications.VGG19(input_shape=(72,72,3),include_top=False,weights='imagenet')\nheadModel = baseModel.output\n\n# headModel = Conv2D(48,3,activation=\"relu\",padding = \"same\")(headModel)\n# # headModel = Conv2D(48,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = MaxPool2D(pool_size = (2,2))(headModel)\n# headModel = BatchNormalization()(headModel)\n# # headModel = Dropout(0.2)(headModel)\n# headModel = Conv2D(96,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = Conv2D(96,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = MaxPool2D(pool_size = (2,2))(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = Dropout(0.15)(headModel)\n\n# headModel = Conv2D(192,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = Conv2D(192,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = Conv2D(192,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = MaxPool2D(pool_size = (2,2))(headModel)\n# headModel = BatchNormalization()(headModel)\n# # headModel = Dropout(0.2)(headModel)\n\n# headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = MaxPool2D(pool_size = (4,4))(headModel)\n# # headModel = BatchNormalization()(headModel)\n# # headModel = Dropout(0.2)(headModel)\n\n\n# headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# # headModel = BatchNormalization()(headModel)\n# headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# # headModel = BatchNormalization()(headModel)\n# headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# # headModel = BatchNormalization()(headModel)\n# headModel = MaxPool2D(pool_size = (2,2),padding = \"valid\")(headModel)\n# headModel = Dropout(0.2)(headModel)\n\n# construct the head of the model that will be placed on top of the\n# the base model\n# headModel = baseModel.output\n# headModel = AveragePooling2D(pool_size=(1, 1))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\n# headModel = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = Dropout(0.15)(headModel)\nheadModel = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\nheadModel = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\nheadModel = Dropout(0.15)(headModel)\nheadModel = Dense(156, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = Dropout(0.1)(headModel)\n# headModel = Dense(256, activation=\"relu\",kernel_initializer = 'he_normal')(headModel)\n# headModel = Dense(1024, activation=\"relu\")(headModel)\n# headModel = Dense(256, activation=\"relu\")(headModel)\nheadModel = Dense(7, activation=\"softmax\")(headModel)\n\n# place the head FC model on top of the base model (this will become\n# the actual model we will train)\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n\n\ncheckpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n                             monitor='val_loss',\n                             mode='min',\n                             save_best_only=True,\n                             verbose=0)\n\n# earlystop = EarlyStopping(monitor='val_loss',\n#                           min_delta=0,\n#                           patience=3,\n#                           verbose=1,\n#                           restore_best_weights=True\n#                           )\n\n# reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n#                               factor=0.1,\n#                               patience=5,\n#                               verbose=1,\n#                               min_delta=0.0001)\n\ncallbacks = [checkpoint]\n\nfor layer in baseModel.layers:\n\tlayer.trainable = False\n\nprint(model.summary())","execution_count":23,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n80142336/80134624 [==============================] - 2s 0us/step\nModel: \"functional_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 72, 72, 3)]       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 72, 72, 64)        1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 72, 72, 64)        36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 36, 36, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 36, 36, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 36, 36, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 18, 18, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 18, 18, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 18, 18, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 18, 18, 256)       590080    \n_________________________________________________________________\nblock3_conv4 (Conv2D)        (None, 18, 18, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 9, 9, 256)         0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 9, 9, 512)         1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock4_conv4 (Conv2D)        (None, 9, 9, 512)         2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n_________________________________________________________________\nblock5_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2048)              0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 512)               1049088   \n_________________________________________________________________\ndense_9 (Dense)              (None, 512)               262656    \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 156)               80028     \n_________________________________________________________________\ndense_11 (Dense)             (None, 7)                 1099      \n=================================================================\nTotal params: 21,417,255\nTrainable params: 1,392,871\nNon-trainable params: 20,024,384\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for layer in baseModel.layers:\n# \tlayer.trainable = False\n\n# compile our model\nprint(\"[INFO] compiling model...\")\nopt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS,verbose=1)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n\n","execution_count":25,"outputs":[{"output_type":"stream","text":"[INFO] compiling model...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the head of the network\nprint(\"[INFO] training head...\")\nH = model.fit(\n\taug.flow(trainX, trainY, batch_size=BS),\n\tsteps_per_epoch=len(trainX) // BS,\n\tvalidation_data=(testX, testY),\n    callbacks=callbacks,\n\tvalidation_steps=len(testX) // BS,\n\tepochs=EPOCHS)\n\nmodel = load_model(\"EmotionDetectionModel.h5\")\n\n\n# make predictions on the testing set\nprint(\"[INFO] evaluating network...\")\npredIdxs = model.predict(testX, batch_size=BS)\nprint(predIdxs)\n# for each image in the testing set we need to find the index of the\n# label with corresponding largest predicted probability\npredIdxs = np.argmax(predIdxs, axis=1)\nprint(predIdxs)\n\n# show a nicely formatted classification report\nprint(classification_report(testY.argmax(axis=1), predIdxs,\n\ttarget_names=lb.classes_))\n\n# serialize the model to disk\nprint(\"[INFO] saving mask detector model...\")\nmodel.save(\"/kaggle/working/model.h5\", save_format=\"h5\")\n\n","execution_count":null,"outputs":[{"output_type":"stream","text":"[INFO] training head...\nEpoch 1/200\n116/116 [==============================] - 46s 394ms/step - loss: 3.9752 - accuracy: 0.2769 - val_loss: 2.9633 - val_accuracy: 0.3543\nEpoch 2/200\n116/116 [==============================] - 44s 379ms/step - loss: 2.8584 - accuracy: 0.3499 - val_loss: 2.6699 - val_accuracy: 0.3883\nEpoch 3/200\n116/116 [==============================] - 44s 383ms/step - loss: 2.6091 - accuracy: 0.3753 - val_loss: 2.4637 - val_accuracy: 0.4017\nEpoch 4/200\n116/116 [==============================] - 44s 381ms/step - loss: 2.4271 - accuracy: 0.3854 - val_loss: 2.3024 - val_accuracy: 0.4154\nEpoch 5/200\n116/116 [==============================] - 45s 384ms/step - loss: 2.2779 - accuracy: 0.4019 - val_loss: 2.1717 - val_accuracy: 0.4209\nEpoch 6/200\n116/116 [==============================] - 43s 374ms/step - loss: 2.1726 - accuracy: 0.4026 - val_loss: 2.0882 - val_accuracy: 0.4268\nEpoch 7/200\n116/116 [==============================] - 44s 380ms/step - loss: 2.0857 - accuracy: 0.4057 - val_loss: 1.9945 - val_accuracy: 0.4344\nEpoch 8/200\n116/116 [==============================] - 45s 384ms/step - loss: 2.0064 - accuracy: 0.4132 - val_loss: 1.9363 - val_accuracy: 0.4368\nEpoch 9/200\n116/116 [==============================] - 44s 376ms/step - loss: 1.9435 - accuracy: 0.4190 - val_loss: 1.8848 - val_accuracy: 0.4429\nEpoch 10/200\n116/116 [==============================] - 44s 383ms/step - loss: 1.9021 - accuracy: 0.4167 - val_loss: 1.8633 - val_accuracy: 0.4395\nEpoch 11/200\n116/116 [==============================] - 44s 379ms/step - loss: 1.8512 - accuracy: 0.4228 - val_loss: 1.8173 - val_accuracy: 0.4374\nEpoch 12/200\n116/116 [==============================] - 45s 385ms/step - loss: 1.8159 - accuracy: 0.4274 - val_loss: 1.7788 - val_accuracy: 0.4401\nEpoch 13/200\n116/116 [==============================] - 44s 375ms/step - loss: 1.7864 - accuracy: 0.4282 - val_loss: 1.7356 - val_accuracy: 0.4486\nEpoch 14/200\n116/116 [==============================] - 44s 377ms/step - loss: 1.7496 - accuracy: 0.4331 - val_loss: 1.7131 - val_accuracy: 0.4466\nEpoch 15/200\n116/116 [==============================] - 44s 380ms/step - loss: 1.7373 - accuracy: 0.4317 - val_loss: 1.6994 - val_accuracy: 0.4550\nEpoch 16/200\n116/116 [==============================] - 44s 382ms/step - loss: 1.7195 - accuracy: 0.4305 - val_loss: 1.6786 - val_accuracy: 0.4480\nEpoch 17/200\n116/116 [==============================] - 43s 372ms/step - loss: 1.6920 - accuracy: 0.4329 - val_loss: 1.6535 - val_accuracy: 0.4489\nEpoch 18/200\n116/116 [==============================] - 44s 380ms/step - loss: 1.6756 - accuracy: 0.4358 - val_loss: 1.6597 - val_accuracy: 0.4381\nEpoch 19/200\n116/116 [==============================] - 44s 380ms/step - loss: 1.6631 - accuracy: 0.4337 - val_loss: 1.6276 - val_accuracy: 0.4542\nEpoch 20/200\n116/116 [==============================] - 44s 379ms/step - loss: 1.6440 - accuracy: 0.4383 - val_loss: 1.6141 - val_accuracy: 0.4546\nEpoch 21/200\n116/116 [==============================] - 44s 377ms/step - loss: 1.6375 - accuracy: 0.4380 - val_loss: 1.5961 - val_accuracy: 0.4565\nEpoch 22/200\n116/116 [==============================] - 43s 374ms/step - loss: 1.6236 - accuracy: 0.4373 - val_loss: 1.6026 - val_accuracy: 0.4442\nEpoch 23/200\n116/116 [==============================] - 44s 376ms/step - loss: 1.6203 - accuracy: 0.4345 - val_loss: 1.5775 - val_accuracy: 0.4605\nEpoch 24/200\n116/116 [==============================] - 44s 377ms/step - loss: 1.6034 - accuracy: 0.4436 - val_loss: 1.5693 - val_accuracy: 0.4616\nEpoch 25/200\n116/116 [==============================] - 43s 367ms/step - loss: 1.6053 - accuracy: 0.4385 - val_loss: 1.5696 - val_accuracy: 0.4533\nEpoch 26/200\n116/116 [==============================] - 44s 384ms/step - loss: 1.5949 - accuracy: 0.4428 - val_loss: 1.5733 - val_accuracy: 0.4605\nEpoch 27/200\n116/116 [==============================] - 44s 375ms/step - loss: 1.5874 - accuracy: 0.4415 - val_loss: 1.5435 - val_accuracy: 0.4612\nEpoch 28/200\n116/116 [==============================] - 43s 372ms/step - loss: 1.5868 - accuracy: 0.4414 - val_loss: 1.5671 - val_accuracy: 0.4667\nEpoch 29/200\n116/116 [==============================] - 44s 375ms/step - loss: 1.5813 - accuracy: 0.4438 - val_loss: 1.5647 - val_accuracy: 0.4521\nEpoch 30/200\n116/116 [==============================] - 44s 380ms/step - loss: 1.5768 - accuracy: 0.4429 - val_loss: 1.5643 - val_accuracy: 0.4508\nEpoch 31/200\n116/116 [==============================] - 44s 380ms/step - loss: 1.5701 - accuracy: 0.4465 - val_loss: 1.5685 - val_accuracy: 0.4562\nEpoch 32/200\n116/116 [==============================] - 44s 375ms/step - loss: 1.5684 - accuracy: 0.4442 - val_loss: 1.5431 - val_accuracy: 0.4533\nEpoch 33/200\n116/116 [==============================] - 44s 376ms/step - loss: 1.5696 - accuracy: 0.4395 - val_loss: 1.5455 - val_accuracy: 0.4599\nEpoch 34/200\n116/116 [==============================] - 44s 377ms/step - loss: 1.5653 - accuracy: 0.4482 - val_loss: 1.5513 - val_accuracy: 0.4498\nEpoch 35/200\n116/116 [==============================] - 44s 378ms/step - loss: 1.5637 - accuracy: 0.4434 - val_loss: 1.5396 - val_accuracy: 0.4601\nEpoch 36/200\n116/116 [==============================] - 43s 371ms/step - loss: 1.5604 - accuracy: 0.4450 - val_loss: 1.5480 - val_accuracy: 0.4497\nEpoch 37/200\n116/116 [==============================] - 45s 388ms/step - loss: 1.5527 - accuracy: 0.4444 - val_loss: 1.5341 - val_accuracy: 0.4516\nEpoch 38/200\n116/116 [==============================] - 45s 385ms/step - loss: 1.5509 - accuracy: 0.4464 - val_loss: 1.5157 - val_accuracy: 0.4682\nEpoch 39/200\n116/116 [==============================] - 44s 382ms/step - loss: 1.5519 - accuracy: 0.4441 - val_loss: 1.5328 - val_accuracy: 0.4580\nEpoch 40/200\n116/116 [==============================] - 43s 374ms/step - loss: 1.5453 - accuracy: 0.4456 - val_loss: 1.5383 - val_accuracy: 0.4564\nEpoch 41/200\n116/116 [==============================] - 44s 376ms/step - loss: 1.5504 - accuracy: 0.4446 - val_loss: 1.5343 - val_accuracy: 0.4552\nEpoch 42/200\n116/116 [==============================] - 44s 379ms/step - loss: 1.5452 - accuracy: 0.4485 - val_loss: 1.5354 - val_accuracy: 0.4548\nEpoch 43/200\n116/116 [==============================] - ETA: 0s - loss: 1.5468 - accuracy: 0.4457\nEpoch 00043: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n116/116 [==============================] - 44s 380ms/step - loss: 1.5468 - accuracy: 0.4457 - val_loss: 1.5220 - val_accuracy: 0.4628\nEpoch 44/200\n116/116 [==============================] - 43s 374ms/step - loss: 1.5131 - accuracy: 0.4592 - val_loss: 1.4992 - val_accuracy: 0.4728\nEpoch 45/200\n116/116 [==============================] - 44s 381ms/step - loss: 1.4930 - accuracy: 0.4659 - val_loss: 1.4862 - val_accuracy: 0.4781\nEpoch 46/200\n116/116 [==============================] - 45s 388ms/step - loss: 1.4825 - accuracy: 0.4700 - val_loss: 1.4813 - val_accuracy: 0.4761\nEpoch 47/200\n116/116 [==============================] - 43s 371ms/step - loss: 1.4751 - accuracy: 0.4709 - val_loss: 1.4817 - val_accuracy: 0.4807\nEpoch 48/200\n116/116 [==============================] - 43s 370ms/step - loss: 1.4682 - accuracy: 0.4755 - val_loss: 1.4729 - val_accuracy: 0.4782\nEpoch 49/200\n116/116 [==============================] - 44s 377ms/step - loss: 1.4685 - accuracy: 0.4725 - val_loss: 1.4748 - val_accuracy: 0.4740\nEpoch 50/200\n116/116 [==============================] - 44s 379ms/step - loss: 1.4643 - accuracy: 0.4751 - val_loss: 1.4682 - val_accuracy: 0.4821\nEpoch 51/200\n116/116 [==============================] - 43s 374ms/step - loss: 1.4621 - accuracy: 0.4778 - val_loss: 1.4612 - val_accuracy: 0.4833\nEpoch 52/200\n116/116 [==============================] - 44s 375ms/step - loss: 1.4559 - accuracy: 0.4781 - val_loss: 1.4707 - val_accuracy: 0.4748\nEpoch 53/200\n116/116 [==============================] - 44s 375ms/step - loss: 1.4489 - accuracy: 0.4817 - val_loss: 1.4688 - val_accuracy: 0.4785\nEpoch 54/200\n116/116 [==============================] - 44s 380ms/step - loss: 1.4468 - accuracy: 0.4773 - val_loss: 1.4689 - val_accuracy: 0.4731\nEpoch 55/200\n","name":"stdout"},{"output_type":"stream","text":"116/116 [==============================] - 43s 367ms/step - loss: 1.4441 - accuracy: 0.4818 - val_loss: 1.4775 - val_accuracy: 0.4779\nEpoch 56/200\n116/116 [==============================] - ETA: 0s - loss: 1.4451 - accuracy: 0.4846\nEpoch 00056: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n116/116 [==============================] - 44s 377ms/step - loss: 1.4451 - accuracy: 0.4846 - val_loss: 1.4632 - val_accuracy: 0.4744\nEpoch 57/200\n116/116 [==============================] - 44s 383ms/step - loss: 1.4212 - accuracy: 0.4899 - val_loss: 1.4578 - val_accuracy: 0.4798\nEpoch 58/200\n116/116 [==============================] - 44s 379ms/step - loss: 1.4249 - accuracy: 0.4918 - val_loss: 1.4558 - val_accuracy: 0.4802\nEpoch 59/200\n116/116 [==============================] - 43s 368ms/step - loss: 1.4249 - accuracy: 0.4891 - val_loss: 1.4590 - val_accuracy: 0.4813\nEpoch 60/200\n116/116 [==============================] - 44s 380ms/step - loss: 1.4275 - accuracy: 0.4865 - val_loss: 1.4533 - val_accuracy: 0.4809\nEpoch 61/200\n116/116 [==============================] - 44s 380ms/step - loss: 1.4193 - accuracy: 0.4919 - val_loss: 1.4535 - val_accuracy: 0.4821\nEpoch 62/200\n116/116 [==============================] - 43s 374ms/step - loss: 1.4240 - accuracy: 0.4896 - val_loss: 1.4520 - val_accuracy: 0.4834\nEpoch 63/200\n116/116 [==============================] - 43s 372ms/step - loss: 1.4182 - accuracy: 0.4889 - val_loss: 1.4526 - val_accuracy: 0.4830\nEpoch 64/200\n116/116 [==============================] - 44s 379ms/step - loss: 1.4109 - accuracy: 0.4959 - val_loss: 1.4517 - val_accuracy: 0.4849\nEpoch 65/200\n116/116 [==============================] - 44s 379ms/step - loss: 1.4140 - accuracy: 0.4898 - val_loss: 1.4491 - val_accuracy: 0.4857\nEpoch 66/200\n116/116 [==============================] - 43s 373ms/step - loss: 1.4091 - accuracy: 0.4943 - val_loss: 1.4540 - val_accuracy: 0.4888\nEpoch 67/200\n116/116 [==============================] - 43s 370ms/step - loss: 1.4124 - accuracy: 0.4952 - val_loss: 1.4519 - val_accuracy: 0.4846\nEpoch 68/200\n116/116 [==============================] - 45s 385ms/step - loss: 1.4128 - accuracy: 0.4930 - val_loss: 1.4485 - val_accuracy: 0.4853\nEpoch 69/200\n116/116 [==============================] - 44s 375ms/step - loss: 1.4042 - accuracy: 0.4957 - val_loss: 1.4518 - val_accuracy: 0.4862\nEpoch 70/200\n116/116 [==============================] - 43s 369ms/step - loss: 1.4054 - accuracy: 0.4957 - val_loss: 1.4497 - val_accuracy: 0.4841\nEpoch 71/200\n116/116 [==============================] - 44s 381ms/step - loss: 1.4033 - accuracy: 0.4965 - val_loss: 1.4478 - val_accuracy: 0.4862\nEpoch 72/200\n116/116 [==============================] - 44s 377ms/step - loss: 1.4052 - accuracy: 0.4947 - val_loss: 1.4475 - val_accuracy: 0.4855\nEpoch 73/200\n 39/116 [=========>....................] - ETA: 26s - loss: 1.4108 - accuracy: 0.4902","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = H.history\nprint(history_dict.keys())\n\n# plot the training loss and accuracy\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_accuracy\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.title(\"Training Loss and Accuracy for VGG19\")\nplt.xlabel(f\"Epoch {EPOCHS}\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(\"/kaggle/working/plot\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}