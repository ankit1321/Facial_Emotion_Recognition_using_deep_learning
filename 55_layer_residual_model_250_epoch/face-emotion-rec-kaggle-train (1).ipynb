{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"! pip install imutils","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting imutils\n  Downloading imutils-0.5.4.tar.gz (17 kB)\nBuilding wheels for collected packages: imutils\n  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25858 sha256=8277c2ada3b3bd70a9dab6b8df48232754e5a5a4149f2a929a05c264740492ff\n  Stored in directory: /root/.cache/pip/wheels/86/d7/0a/4923351ed1cec5d5e24c1eaf8905567b02a0343b24aa873df2\nSuccessfully built imutils\nInstalling collected packages: imutils\nSuccessfully installed imutils-0.5.4\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\n# from tensorflow.keras.models import load_model\n# from keras.layers import LeakyReLU\nfrom sklearn.preprocessing import LabelBinarizer,LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport os","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[INFO] loading images...\")\nimagePaths = list(paths.list_images(\"../input/human-anime/human_anime\"))\n# print(imagePaths)\ndata = []\nlabels = []","execution_count":3,"outputs":[{"output_type":"stream","text":"[INFO] loading images...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for imagePath in imagePaths:\n\t# extract the class label from the filename\n\tlabel = imagePath.split(os.path.sep)[-2]\n\n\t# load the input image (224x224) and preprocess it\n\timage = load_img(imagePath, target_size=(72, 72))\n\timage = img_to_array(image)\n# \timage = preprocess_input(image)\n\t# update the data and labels lists, respectively\n\t# count+=1/\n\t# if count%10==0:\n\tdata.append(image)\n\tlabels.append(label)\n    \n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.array(data, dtype=\"float32\")\nlabels1 = np.array(labels)\ndel labels\nprint(\"labels\",labels1.shape)\n# perform one-hot encoding on the labels\nlb = LabelEncoder()\nlabels = lb.fit_transform(labels1)\nlabels = to_categorical(labels)\nprint(labels,type(labels))","execution_count":5,"outputs":[{"output_type":"stream","text":"labels (37369,)\n[[0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 0. 1.]\n [0. 0. 0. ... 0. 0. 1.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]] <class 'numpy.ndarray'>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape,labels.shape)\n(trainX, testX, trainY, testY) = train_test_split(data, labels,\n\ttest_size=0.2, stratify=labels, random_state=42)\nprint(trainX.shape, testX.shape, trainY.shape, testY.shape)","execution_count":6,"outputs":[{"output_type":"stream","text":"(37369, 72, 72, 3) (37369, 7)\n(29895, 72, 72, 3) (7474, 72, 72, 3) (29895, 7) (7474, 7)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = ImageDataGenerator(\n\trotation_range=20,\n\tzoom_range=0.15,\n\twidth_shift_range=0.2,\n\theight_shift_range=0.2,\n\tshear_range=0.15,\n\thorizontal_flip=True,\n\tfill_mode=\"nearest\")","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install keras-resnet","execution_count":16,"outputs":[{"output_type":"stream","text":"Collecting keras-resnet\n  Downloading keras-resnet-0.2.0.tar.gz (9.3 kB)\nRequirement already satisfied: keras>=2.2.4 in /opt/conda/lib/python3.7/site-packages (from keras-resnet) (2.4.3)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras>=2.2.4->keras-resnet) (1.18.5)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras>=2.2.4->keras-resnet) (1.4.1)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras>=2.2.4->keras-resnet) (2.10.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras>=2.2.4->keras-resnet) (5.3.1)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras>=2.2.4->keras-resnet) (1.18.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras>=2.2.4->keras-resnet) (1.14.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras>=2.2.4->keras-resnet) (1.18.5)\nBuilding wheels for collected packages: keras-resnet\n  Building wheel for keras-resnet (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=20487 sha256=7bac0ee41922b176dcb8608dfcbb0b12a408184fc999444917145233f6240296\n  Stored in directory: /root/.cache/pip/wheels/bd/ef/06/5d65f696360436c3a423020c4b7fd8c558c09ef264a0e6c575\nSuccessfully built keras-resnet\nInstalling collected packages: keras-resnet\nSuccessfully installed keras-resnet-0.2.0\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import InputLayer,Conv2D,MaxPool2D,BatchNormalization,ZeroPadding2D,ReLU\nfrom keras import regularizers\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.models import load_model\n\nimport numpy as np\nfrom keras import layers\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.models import Model, load_model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nimport pydot\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\n# from resnets_utils import *\nfrom keras.initializers import glorot_uniform\nimport scipy.misc\nfrom matplotlib.pyplot import imshow","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def identity_block(X, f, filters, stage, block):\n    \n    # Defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value\n    X_shortcut = X\n    \n    # First component of main path\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n    \n    # Second component of main path\n    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n    X = Activation('relu')(X)\n\n    # Third component of main path \n    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n\n    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convolutional_block(X, f, filters, stage, block, s=2):\n\n    # Defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    # Retrieve Filters\n    F1, F2, F3 = filters\n\n    # Save the input value\n    X_shortcut = X\n\n    ##### MAIN PATH #####\n    # First component of main path \n    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n\n    # Second component of main path\n    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n    X = Activation('relu')(X)\n\n    # Third component of main path\n    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n\n    ##### SHORTCUT PATH #### \n    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n\n    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n\n    return X","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ResNet50(input_shape = (72, 72, 3), classes = 7):   \n    \n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n\n    \n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n    # Stage 2\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n\n    # Stage 3\n    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n\n    # Stage 4\n    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n\n    # Stage 5\n    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n\n    # AVGPOOL\n    X = AveragePooling2D(pool_size=(2,2), padding='same')(X)\n\n    # Output layer\n    X = Flatten()(X)\n    X = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(X)\n    X = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(X)\n    X = Dropout(0.15)(X)\n    X = Dense(156, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(X)\n    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n    \n    \n    # Create model\n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n\n    return model","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INIT_LR = 1e-4\nEPOCHS = 250\nBS = 128\n\n\n# baseModel = Sequential()\n# # baseModel.add(Dense(12, activation='relu', input_tensor = Input(shape=(48,48,3))))\n# baseModel.add(InputLayer(input_shape = (72,72,3)))\n# # baseModel = tf.keras.applications.Xception(input_shape=(72,72,3),include_top=False,weights='imagenet')\n# headModel = baseModel.output\n# headModel = ZeroPadding2D()(headModel)\n# headModel = Conv2D(48,3,activation=\"relu\")(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = ZeroPadding2D()(headModel)\n# headModel = MaxPool2D(pool_size = (2,2))(headModel)\n# headModel = Conv2D(48,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = Conv2D(96,3,activation=\"relu\",padding = \"same\")(headModel)\n\n\n\n\n\n\n# headModel = Dropout(0.15)(headModel)\n# headModel = Conv2D(96,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = MaxPool2D(pool_size = (2,2))(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = Dropout(0.15)(headModel)\n\n# headModel = Conv2D(192,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = Conv2D(192,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = Conv2D(192,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = MaxPool2D(pool_size = (2,2))(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = Dropout(0.15)(headModel)\n\n# headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# headModel = MaxPool2D(pool_size = (4,4))(headModel)\n# # headModel = BatchNormalization()(headModel)\n# # headModel = Dropout(0.2)(headModel)\n\n\n# # headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# # # headModel = BatchNormalization()(headModel)\n# # headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# # # headModel = BatchNormalization()(headModel)\n# # headModel = Conv2D(384,3,activation=\"relu\",padding = \"same\")(headModel)\n# # # headModel = BatchNormalization()(headModel)\n# # headModel = MaxPool2D(pool_size = (2,2),padding = \"valid\")(headModel)\n# # headModel = Dropout(0.2)(headModel)\n\n# # construct the head of the model that will be placed on top of the\n# # the base model\n# # headModel = baseModel.output\n# # headModel = AveragePooling2D(pool_size=(1, 1))(headModel)\n# headModel = Flatten(name=\"flatten\")(headModel)\n# headModel = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = Dropout(0.15)(headModel)\n# headModel = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = Dropout(0.15)(headModel)\n# headModel = Dense(156, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# # headModel = Dropout(0.1)(headModel)\n# # headModel = Dense(256, activation=\"relu\",kernel_initializer = 'he_normal')(headModel)\n# # headModel = Dense(1024, activation=\"relu\")(headModel)\n# # headModel = Dense(256, activation=\"relu\")(headModel)\n# headModel = Dense(7, activation=\"softmax\")(headModel)\n\n# # place the head FC model on top of the base model (this will become\n# # the actual model we will train)\n# model = Model(inputs=baseModel.input, outputs=headModel)\n\n\n# checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n#                              monitor='val_loss',\n#                              mode='min',\n#                              save_best_only=True,\n#                              verbose=0)\n\n# # earlystop = EarlyStopping(monitor='val_loss',\n# #                           min_delta=0,\n# #                           patience=3,\n# #                           verbose=1,\n# #                           restore_best_weights=True\n# #                           )\n\n# # reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n# #                               factor=0.2,\n# #                               patience=5,\n# #                               verbose=1,\n# #                               min_delta=0.0001)\n\n# callbacks = [checkpoint]\n\n# # for layer in baseModel.layers:\n# # \tlayer.trainable = False\n\n# print(model.summary())","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ResNet50(input_shape = (72, 72, 3), classes = 7)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n                             monitor='val_loss',\n                             mode='min',\n                             save_best_only=True,\n                             verbose=0)\n\ncallbacks = [checkpoint]\nprint(model.summary())","execution_count":44,"outputs":[{"output_type":"stream","text":"Model: \"ResNet50\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_5 (InputLayer)            [(None, 72, 72, 3)]  0                                            \n__________________________________________________________________________________________________\nzero_padding2d_4 (ZeroPadding2D (None, 78, 78, 3)    0           input_5[0][0]                    \n__________________________________________________________________________________________________\nconv1 (Conv2D)                  (None, 36, 36, 64)   9472        zero_padding2d_4[0][0]           \n__________________________________________________________________________________________________\nbn_conv1 (BatchNormalization)   (None, 36, 36, 64)   256         conv1[0][0]                      \n__________________________________________________________________________________________________\nactivation_147 (Activation)     (None, 36, 36, 64)   0           bn_conv1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 64)   0           activation_147[0][0]             \n__________________________________________________________________________________________________\nres2a_branch2a (Conv2D)         (None, 17, 17, 64)   4160        max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nbn2a_branch2a (BatchNormalizati (None, 17, 17, 64)   256         res2a_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_148 (Activation)     (None, 17, 17, 64)   0           bn2a_branch2a[0][0]              \n__________________________________________________________________________________________________\nres2a_branch2b (Conv2D)         (None, 17, 17, 64)   36928       activation_148[0][0]             \n__________________________________________________________________________________________________\nbn2a_branch2b (BatchNormalizati (None, 17, 17, 64)   256         res2a_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_149 (Activation)     (None, 17, 17, 64)   0           bn2a_branch2b[0][0]              \n__________________________________________________________________________________________________\nres2a_branch2c (Conv2D)         (None, 17, 17, 256)  16640       activation_149[0][0]             \n__________________________________________________________________________________________________\nres2a_branch1 (Conv2D)          (None, 17, 17, 256)  16640       max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nbn2a_branch2c (BatchNormalizati (None, 17, 17, 256)  1024        res2a_branch2c[0][0]             \n__________________________________________________________________________________________________\nbn2a_branch1 (BatchNormalizatio (None, 17, 17, 256)  1024        res2a_branch1[0][0]              \n__________________________________________________________________________________________________\nadd_48 (Add)                    (None, 17, 17, 256)  0           bn2a_branch2c[0][0]              \n                                                                 bn2a_branch1[0][0]               \n__________________________________________________________________________________________________\nactivation_150 (Activation)     (None, 17, 17, 256)  0           add_48[0][0]                     \n__________________________________________________________________________________________________\nres2b_branch2a (Conv2D)         (None, 17, 17, 64)   16448       activation_150[0][0]             \n__________________________________________________________________________________________________\nbn2b_branch2a (BatchNormalizati (None, 17, 17, 64)   256         res2b_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_151 (Activation)     (None, 17, 17, 64)   0           bn2b_branch2a[0][0]              \n__________________________________________________________________________________________________\nres2b_branch2b (Conv2D)         (None, 17, 17, 64)   36928       activation_151[0][0]             \n__________________________________________________________________________________________________\nbn2b_branch2b (BatchNormalizati (None, 17, 17, 64)   256         res2b_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_152 (Activation)     (None, 17, 17, 64)   0           bn2b_branch2b[0][0]              \n__________________________________________________________________________________________________\nres2b_branch2c (Conv2D)         (None, 17, 17, 256)  16640       activation_152[0][0]             \n__________________________________________________________________________________________________\nbn2b_branch2c (BatchNormalizati (None, 17, 17, 256)  1024        res2b_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_49 (Add)                    (None, 17, 17, 256)  0           bn2b_branch2c[0][0]              \n                                                                 activation_150[0][0]             \n__________________________________________________________________________________________________\nactivation_153 (Activation)     (None, 17, 17, 256)  0           add_49[0][0]                     \n__________________________________________________________________________________________________\nres2c_branch2a (Conv2D)         (None, 17, 17, 64)   16448       activation_153[0][0]             \n__________________________________________________________________________________________________\nbn2c_branch2a (BatchNormalizati (None, 17, 17, 64)   256         res2c_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_154 (Activation)     (None, 17, 17, 64)   0           bn2c_branch2a[0][0]              \n__________________________________________________________________________________________________\nres2c_branch2b (Conv2D)         (None, 17, 17, 64)   36928       activation_154[0][0]             \n__________________________________________________________________________________________________\nbn2c_branch2b (BatchNormalizati (None, 17, 17, 64)   256         res2c_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_155 (Activation)     (None, 17, 17, 64)   0           bn2c_branch2b[0][0]              \n__________________________________________________________________________________________________\nres2c_branch2c (Conv2D)         (None, 17, 17, 256)  16640       activation_155[0][0]             \n__________________________________________________________________________________________________\nbn2c_branch2c (BatchNormalizati (None, 17, 17, 256)  1024        res2c_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_50 (Add)                    (None, 17, 17, 256)  0           bn2c_branch2c[0][0]              \n                                                                 activation_153[0][0]             \n__________________________________________________________________________________________________\nactivation_156 (Activation)     (None, 17, 17, 256)  0           add_50[0][0]                     \n__________________________________________________________________________________________________\nres3a_branch2a (Conv2D)         (None, 9, 9, 128)    32896       activation_156[0][0]             \n__________________________________________________________________________________________________\nbn3a_branch2a (BatchNormalizati (None, 9, 9, 128)    512         res3a_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_157 (Activation)     (None, 9, 9, 128)    0           bn3a_branch2a[0][0]              \n__________________________________________________________________________________________________\nres3a_branch2b (Conv2D)         (None, 9, 9, 128)    147584      activation_157[0][0]             \n__________________________________________________________________________________________________\nbn3a_branch2b (BatchNormalizati (None, 9, 9, 128)    512         res3a_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_158 (Activation)     (None, 9, 9, 128)    0           bn3a_branch2b[0][0]              \n__________________________________________________________________________________________________\nres3a_branch2c (Conv2D)         (None, 9, 9, 512)    66048       activation_158[0][0]             \n__________________________________________________________________________________________________\nres3a_branch1 (Conv2D)          (None, 9, 9, 512)    131584      activation_156[0][0]             \n__________________________________________________________________________________________________\nbn3a_branch2c (BatchNormalizati (None, 9, 9, 512)    2048        res3a_branch2c[0][0]             \n__________________________________________________________________________________________________\nbn3a_branch1 (BatchNormalizatio (None, 9, 9, 512)    2048        res3a_branch1[0][0]              \n__________________________________________________________________________________________________\nadd_51 (Add)                    (None, 9, 9, 512)    0           bn3a_branch2c[0][0]              \n                                                                 bn3a_branch1[0][0]               \n__________________________________________________________________________________________________\nactivation_159 (Activation)     (None, 9, 9, 512)    0           add_51[0][0]                     \n__________________________________________________________________________________________________\nres3b_branch2a (Conv2D)         (None, 9, 9, 128)    65664       activation_159[0][0]             \n__________________________________________________________________________________________________\nbn3b_branch2a (BatchNormalizati (None, 9, 9, 128)    512         res3b_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_160 (Activation)     (None, 9, 9, 128)    0           bn3b_branch2a[0][0]              \n__________________________________________________________________________________________________\nres3b_branch2b (Conv2D)         (None, 9, 9, 128)    147584      activation_160[0][0]             \n__________________________________________________________________________________________________\nbn3b_branch2b (BatchNormalizati (None, 9, 9, 128)    512         res3b_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_161 (Activation)     (None, 9, 9, 128)    0           bn3b_branch2b[0][0]              \n__________________________________________________________________________________________________\nres3b_branch2c (Conv2D)         (None, 9, 9, 512)    66048       activation_161[0][0]             \n__________________________________________________________________________________________________\nbn3b_branch2c (BatchNormalizati (None, 9, 9, 512)    2048        res3b_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_52 (Add)                    (None, 9, 9, 512)    0           bn3b_branch2c[0][0]              \n                                                                 activation_159[0][0]             \n__________________________________________________________________________________________________\nactivation_162 (Activation)     (None, 9, 9, 512)    0           add_52[0][0]                     \n__________________________________________________________________________________________________\nres3c_branch2a (Conv2D)         (None, 9, 9, 128)    65664       activation_162[0][0]             \n__________________________________________________________________________________________________\nbn3c_branch2a (BatchNormalizati (None, 9, 9, 128)    512         res3c_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_163 (Activation)     (None, 9, 9, 128)    0           bn3c_branch2a[0][0]              \n__________________________________________________________________________________________________\nres3c_branch2b (Conv2D)         (None, 9, 9, 128)    147584      activation_163[0][0]             \n__________________________________________________________________________________________________\nbn3c_branch2b (BatchNormalizati (None, 9, 9, 128)    512         res3c_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_164 (Activation)     (None, 9, 9, 128)    0           bn3c_branch2b[0][0]              \n__________________________________________________________________________________________________\nres3c_branch2c (Conv2D)         (None, 9, 9, 512)    66048       activation_164[0][0]             \n__________________________________________________________________________________________________\nbn3c_branch2c (BatchNormalizati (None, 9, 9, 512)    2048        res3c_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_53 (Add)                    (None, 9, 9, 512)    0           bn3c_branch2c[0][0]              \n                                                                 activation_162[0][0]             \n__________________________________________________________________________________________________\nactivation_165 (Activation)     (None, 9, 9, 512)    0           add_53[0][0]                     \n__________________________________________________________________________________________________\nres3d_branch2a (Conv2D)         (None, 9, 9, 128)    65664       activation_165[0][0]             \n__________________________________________________________________________________________________\nbn3d_branch2a (BatchNormalizati (None, 9, 9, 128)    512         res3d_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_166 (Activation)     (None, 9, 9, 128)    0           bn3d_branch2a[0][0]              \n__________________________________________________________________________________________________\nres3d_branch2b (Conv2D)         (None, 9, 9, 128)    147584      activation_166[0][0]             \n__________________________________________________________________________________________________\nbn3d_branch2b (BatchNormalizati (None, 9, 9, 128)    512         res3d_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_167 (Activation)     (None, 9, 9, 128)    0           bn3d_branch2b[0][0]              \n__________________________________________________________________________________________________\nres3d_branch2c (Conv2D)         (None, 9, 9, 512)    66048       activation_167[0][0]             \n__________________________________________________________________________________________________\nbn3d_branch2c (BatchNormalizati (None, 9, 9, 512)    2048        res3d_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_54 (Add)                    (None, 9, 9, 512)    0           bn3d_branch2c[0][0]              \n                                                                 activation_165[0][0]             \n__________________________________________________________________________________________________\nactivation_168 (Activation)     (None, 9, 9, 512)    0           add_54[0][0]                     \n__________________________________________________________________________________________________\nres4a_branch2a (Conv2D)         (None, 5, 5, 256)    131328      activation_168[0][0]             \n__________________________________________________________________________________________________\nbn4a_branch2a (BatchNormalizati (None, 5, 5, 256)    1024        res4a_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_169 (Activation)     (None, 5, 5, 256)    0           bn4a_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4a_branch2b (Conv2D)         (None, 5, 5, 256)    590080      activation_169[0][0]             \n__________________________________________________________________________________________________\nbn4a_branch2b (BatchNormalizati (None, 5, 5, 256)    1024        res4a_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_170 (Activation)     (None, 5, 5, 256)    0           bn4a_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4a_branch2c (Conv2D)         (None, 5, 5, 1024)   263168      activation_170[0][0]             \n__________________________________________________________________________________________________\nres4a_branch1 (Conv2D)          (None, 5, 5, 1024)   525312      activation_168[0][0]             \n__________________________________________________________________________________________________\nbn4a_branch2c (BatchNormalizati (None, 5, 5, 1024)   4096        res4a_branch2c[0][0]             \n__________________________________________________________________________________________________\nbn4a_branch1 (BatchNormalizatio (None, 5, 5, 1024)   4096        res4a_branch1[0][0]              \n__________________________________________________________________________________________________\nadd_55 (Add)                    (None, 5, 5, 1024)   0           bn4a_branch2c[0][0]              \n                                                                 bn4a_branch1[0][0]               \n__________________________________________________________________________________________________\nactivation_171 (Activation)     (None, 5, 5, 1024)   0           add_55[0][0]                     \n__________________________________________________________________________________________________\nres4b_branch2a (Conv2D)         (None, 5, 5, 256)    262400      activation_171[0][0]             \n__________________________________________________________________________________________________\nbn4b_branch2a (BatchNormalizati (None, 5, 5, 256)    1024        res4b_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_172 (Activation)     (None, 5, 5, 256)    0           bn4b_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4b_branch2b (Conv2D)         (None, 5, 5, 256)    590080      activation_172[0][0]             \n__________________________________________________________________________________________________\nbn4b_branch2b (BatchNormalizati (None, 5, 5, 256)    1024        res4b_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_173 (Activation)     (None, 5, 5, 256)    0           bn4b_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4b_branch2c (Conv2D)         (None, 5, 5, 1024)   263168      activation_173[0][0]             \n__________________________________________________________________________________________________\nbn4b_branch2c (BatchNormalizati (None, 5, 5, 1024)   4096        res4b_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_56 (Add)                    (None, 5, 5, 1024)   0           bn4b_branch2c[0][0]              \n                                                                 activation_171[0][0]             \n__________________________________________________________________________________________________\nactivation_174 (Activation)     (None, 5, 5, 1024)   0           add_56[0][0]                     \n__________________________________________________________________________________________________\nres4c_branch2a (Conv2D)         (None, 5, 5, 256)    262400      activation_174[0][0]             \n__________________________________________________________________________________________________\nbn4c_branch2a (BatchNormalizati (None, 5, 5, 256)    1024        res4c_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_175 (Activation)     (None, 5, 5, 256)    0           bn4c_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4c_branch2b (Conv2D)         (None, 5, 5, 256)    590080      activation_175[0][0]             \n__________________________________________________________________________________________________\nbn4c_branch2b (BatchNormalizati (None, 5, 5, 256)    1024        res4c_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_176 (Activation)     (None, 5, 5, 256)    0           bn4c_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4c_branch2c (Conv2D)         (None, 5, 5, 1024)   263168      activation_176[0][0]             \n__________________________________________________________________________________________________\nbn4c_branch2c (BatchNormalizati (None, 5, 5, 1024)   4096        res4c_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_57 (Add)                    (None, 5, 5, 1024)   0           bn4c_branch2c[0][0]              \n                                                                 activation_174[0][0]             \n__________________________________________________________________________________________________\nactivation_177 (Activation)     (None, 5, 5, 1024)   0           add_57[0][0]                     \n__________________________________________________________________________________________________\nres4d_branch2a (Conv2D)         (None, 5, 5, 256)    262400      activation_177[0][0]             \n__________________________________________________________________________________________________\nbn4d_branch2a (BatchNormalizati (None, 5, 5, 256)    1024        res4d_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_178 (Activation)     (None, 5, 5, 256)    0           bn4d_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4d_branch2b (Conv2D)         (None, 5, 5, 256)    590080      activation_178[0][0]             \n__________________________________________________________________________________________________\nbn4d_branch2b (BatchNormalizati (None, 5, 5, 256)    1024        res4d_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_179 (Activation)     (None, 5, 5, 256)    0           bn4d_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4d_branch2c (Conv2D)         (None, 5, 5, 1024)   263168      activation_179[0][0]             \n__________________________________________________________________________________________________\nbn4d_branch2c (BatchNormalizati (None, 5, 5, 1024)   4096        res4d_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_58 (Add)                    (None, 5, 5, 1024)   0           bn4d_branch2c[0][0]              \n                                                                 activation_177[0][0]             \n__________________________________________________________________________________________________\nactivation_180 (Activation)     (None, 5, 5, 1024)   0           add_58[0][0]                     \n__________________________________________________________________________________________________\nres4e_branch2a (Conv2D)         (None, 5, 5, 256)    262400      activation_180[0][0]             \n__________________________________________________________________________________________________\nbn4e_branch2a (BatchNormalizati (None, 5, 5, 256)    1024        res4e_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_181 (Activation)     (None, 5, 5, 256)    0           bn4e_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4e_branch2b (Conv2D)         (None, 5, 5, 256)    590080      activation_181[0][0]             \n__________________________________________________________________________________________________\nbn4e_branch2b (BatchNormalizati (None, 5, 5, 256)    1024        res4e_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_182 (Activation)     (None, 5, 5, 256)    0           bn4e_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4e_branch2c (Conv2D)         (None, 5, 5, 1024)   263168      activation_182[0][0]             \n__________________________________________________________________________________________________\nbn4e_branch2c (BatchNormalizati (None, 5, 5, 1024)   4096        res4e_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_59 (Add)                    (None, 5, 5, 1024)   0           bn4e_branch2c[0][0]              \n                                                                 activation_180[0][0]             \n__________________________________________________________________________________________________\nactivation_183 (Activation)     (None, 5, 5, 1024)   0           add_59[0][0]                     \n__________________________________________________________________________________________________\nres4f_branch2a (Conv2D)         (None, 5, 5, 256)    262400      activation_183[0][0]             \n__________________________________________________________________________________________________\nbn4f_branch2a (BatchNormalizati (None, 5, 5, 256)    1024        res4f_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_184 (Activation)     (None, 5, 5, 256)    0           bn4f_branch2a[0][0]              \n__________________________________________________________________________________________________\nres4f_branch2b (Conv2D)         (None, 5, 5, 256)    590080      activation_184[0][0]             \n__________________________________________________________________________________________________\nbn4f_branch2b (BatchNormalizati (None, 5, 5, 256)    1024        res4f_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_185 (Activation)     (None, 5, 5, 256)    0           bn4f_branch2b[0][0]              \n__________________________________________________________________________________________________\nres4f_branch2c (Conv2D)         (None, 5, 5, 1024)   263168      activation_185[0][0]             \n__________________________________________________________________________________________________\nbn4f_branch2c (BatchNormalizati (None, 5, 5, 1024)   4096        res4f_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_60 (Add)                    (None, 5, 5, 1024)   0           bn4f_branch2c[0][0]              \n                                                                 activation_183[0][0]             \n__________________________________________________________________________________________________\nactivation_186 (Activation)     (None, 5, 5, 1024)   0           add_60[0][0]                     \n__________________________________________________________________________________________________\nres5a_branch2a (Conv2D)         (None, 3, 3, 512)    524800      activation_186[0][0]             \n__________________________________________________________________________________________________\nbn5a_branch2a (BatchNormalizati (None, 3, 3, 512)    2048        res5a_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_187 (Activation)     (None, 3, 3, 512)    0           bn5a_branch2a[0][0]              \n__________________________________________________________________________________________________\nres5a_branch2b (Conv2D)         (None, 3, 3, 512)    2359808     activation_187[0][0]             \n__________________________________________________________________________________________________\nbn5a_branch2b (BatchNormalizati (None, 3, 3, 512)    2048        res5a_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_188 (Activation)     (None, 3, 3, 512)    0           bn5a_branch2b[0][0]              \n__________________________________________________________________________________________________\nres5a_branch2c (Conv2D)         (None, 3, 3, 2048)   1050624     activation_188[0][0]             \n__________________________________________________________________________________________________\nres5a_branch1 (Conv2D)          (None, 3, 3, 2048)   2099200     activation_186[0][0]             \n__________________________________________________________________________________________________\nbn5a_branch2c (BatchNormalizati (None, 3, 3, 2048)   8192        res5a_branch2c[0][0]             \n__________________________________________________________________________________________________\nbn5a_branch1 (BatchNormalizatio (None, 3, 3, 2048)   8192        res5a_branch1[0][0]              \n__________________________________________________________________________________________________\nadd_61 (Add)                    (None, 3, 3, 2048)   0           bn5a_branch2c[0][0]              \n                                                                 bn5a_branch1[0][0]               \n__________________________________________________________________________________________________\nactivation_189 (Activation)     (None, 3, 3, 2048)   0           add_61[0][0]                     \n__________________________________________________________________________________________________\nres5b_branch2a (Conv2D)         (None, 3, 3, 512)    1049088     activation_189[0][0]             \n__________________________________________________________________________________________________\nbn5b_branch2a (BatchNormalizati (None, 3, 3, 512)    2048        res5b_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_190 (Activation)     (None, 3, 3, 512)    0           bn5b_branch2a[0][0]              \n__________________________________________________________________________________________________\nres5b_branch2b (Conv2D)         (None, 3, 3, 512)    2359808     activation_190[0][0]             \n__________________________________________________________________________________________________\nbn5b_branch2b (BatchNormalizati (None, 3, 3, 512)    2048        res5b_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_191 (Activation)     (None, 3, 3, 512)    0           bn5b_branch2b[0][0]              \n__________________________________________________________________________________________________\nres5b_branch2c (Conv2D)         (None, 3, 3, 2048)   1050624     activation_191[0][0]             \n__________________________________________________________________________________________________\nbn5b_branch2c (BatchNormalizati (None, 3, 3, 2048)   8192        res5b_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_62 (Add)                    (None, 3, 3, 2048)   0           bn5b_branch2c[0][0]              \n                                                                 activation_189[0][0]             \n__________________________________________________________________________________________________\nactivation_192 (Activation)     (None, 3, 3, 2048)   0           add_62[0][0]                     \n__________________________________________________________________________________________________\nres5c_branch2a (Conv2D)         (None, 3, 3, 512)    1049088     activation_192[0][0]             \n__________________________________________________________________________________________________\nbn5c_branch2a (BatchNormalizati (None, 3, 3, 512)    2048        res5c_branch2a[0][0]             \n__________________________________________________________________________________________________\nactivation_193 (Activation)     (None, 3, 3, 512)    0           bn5c_branch2a[0][0]              \n__________________________________________________________________________________________________\nres5c_branch2b (Conv2D)         (None, 3, 3, 512)    2359808     activation_193[0][0]             \n__________________________________________________________________________________________________\nbn5c_branch2b (BatchNormalizati (None, 3, 3, 512)    2048        res5c_branch2b[0][0]             \n__________________________________________________________________________________________________\nactivation_194 (Activation)     (None, 3, 3, 512)    0           bn5c_branch2b[0][0]              \n__________________________________________________________________________________________________\nres5c_branch2c (Conv2D)         (None, 3, 3, 2048)   1050624     activation_194[0][0]             \n__________________________________________________________________________________________________\nbn5c_branch2c (BatchNormalizati (None, 3, 3, 2048)   8192        res5c_branch2c[0][0]             \n__________________________________________________________________________________________________\nadd_63 (Add)                    (None, 3, 3, 2048)   0           bn5c_branch2c[0][0]              \n                                                                 activation_192[0][0]             \n__________________________________________________________________________________________________\nactivation_195 (Activation)     (None, 3, 3, 2048)   0           add_63[0][0]                     \n__________________________________________________________________________________________________\naverage_pooling2d_3 (AveragePoo (None, 2, 2, 2048)   0           activation_195[0][0]             \n__________________________________________________________________________________________________\nflatten_3 (Flatten)             (None, 8192)         0           average_pooling2d_3[0][0]        \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 512)          4194816     flatten_3[0][0]                  \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 512)          262656      dense_4[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 512)          0           dense_5[0][0]                    \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 156)          80028       dropout_2[0][0]                  \n__________________________________________________________________________________________________\nfc7 (Dense)                     (None, 7)            1099        dense_6[0][0]                    \n==================================================================================================\nTotal params: 28,126,311\nTrainable params: 28,073,191\nNon-trainable params: 53,120\n__________________________________________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for layer in baseModel.layers:\n# \tlayer.trainable = False\n\n# compile our model\nprint(\"[INFO] compiling model...\")\nopt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n\n","execution_count":45,"outputs":[{"output_type":"stream","text":"[INFO] compiling model...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the head of the network\nprint(\"[INFO] training head...\")\nH = model.fit(\n\taug.flow(trainX, trainY, batch_size=BS),\n\tsteps_per_epoch=len(trainX) // BS,\n\tvalidation_data=(testX, testY),\n    callbacks=callbacks,\n\tvalidation_steps=len(testX) // BS,\n\tepochs=EPOCHS)\n\nmodel = load_model(\"EmotionDetectionModel.h5\")\n\n\n# make predictions on the testing set\nprint(\"[INFO] evaluating network...\")\npredIdxs = model.predict(testX, batch_size=BS)\nprint(predIdxs)\n# for each image in the testing set we need to find the index of the\n# label with corresponding largest predicted probability\npredIdxs = np.argmax(predIdxs, axis=1)\nprint(predIdxs)\n\n# show a nicely formatted classification report\nprint(classification_report(testY.argmax(axis=1), predIdxs,\n\ttarget_names=lb.classes_))\n\n# serialize the model to disk\nprint(\"[INFO] saving mask detector model...\")\nmodel.save(\"/kaggle/working/model.h5\", save_format=\"h5\")\n\n","execution_count":46,"outputs":[{"output_type":"stream","text":"[INFO] training head...\nEpoch 1/250\n233/233 [==============================] - 56s 241ms/step - loss: 3.4547 - accuracy: 0.2326 - val_loss: 3.2458 - val_accuracy: 0.2482\nEpoch 2/250\n233/233 [==============================] - 53s 229ms/step - loss: 3.1072 - accuracy: 0.2426 - val_loss: 2.9723 - val_accuracy: 0.2473\nEpoch 3/250\n233/233 [==============================] - 53s 229ms/step - loss: 2.8722 - accuracy: 0.2510 - val_loss: 2.7906 - val_accuracy: 0.2424A: 20s - loss: 2.9165 - accu - ETA: 17s - lo - ETA - ETA: 1s - loss: 2.8759 \nEpoch 4/250\n233/233 [==============================] - 53s 228ms/step - loss: 2.6799 - accuracy: 0.2607 - val_loss: 2.6244 - val_accuracy: 0.2485\nEpoch 5/250\n233/233 [==============================] - 54s 230ms/step - loss: 2.5096 - accuracy: 0.2798 - val_loss: 2.5678 - val_accuracy: 0.2576\nEpoch 6/250\n233/233 [==============================] - 54s 231ms/step - loss: 2.3531 - accuracy: 0.3059 - val_loss: 2.2929 - val_accuracy: 0.3229\nEpoch 7/250\n233/233 [==============================] - 54s 231ms/step - loss: 2.2198 - accuracy: 0.3311 - val_loss: 2.1777 - val_accuracy: 0.3393\nEpoch 8/250\n233/233 [==============================] - 53s 229ms/step - loss: 2.0987 - accuracy: 0.3553 - val_loss: 2.1270 - val_accuracy: 0.3255\nEpoch 9/250\n233/233 [==============================] - 54s 232ms/step - loss: 1.9888 - accuracy: 0.3793 - val_loss: 2.0098 - val_accuracy: 0.3675\nEpoch 10/250\n233/233 [==============================] - 52s 225ms/step - loss: 1.8979 - accuracy: 0.3954 - val_loss: 2.0335 - val_accuracy: 0.3441\nEpoch 11/250\n233/233 [==============================] - 54s 230ms/step - loss: 1.8050 - accuracy: 0.4156 - val_loss: 1.8056 - val_accuracy: 0.4154\nEpoch 12/250\n233/233 [==============================] - 53s 226ms/step - loss: 1.7307 - accuracy: 0.4310 - val_loss: 1.7412 - val_accuracy: 0.4316\nEpoch 13/250\n233/233 [==============================] - 52s 222ms/step - loss: 1.6644 - accuracy: 0.4468 - val_loss: 1.9586 - val_accuracy: 0.3551\nEpoch 14/250\n233/233 [==============================] - 52s 225ms/step - loss: 1.6107 - accuracy: 0.4557 - val_loss: 2.5769 - val_accuracy: 0.3172\nEpoch 15/250\n233/233 [==============================] - 53s 229ms/step - loss: 1.5468 - accuracy: 0.4757 - val_loss: 1.6097 - val_accuracy: 0.4493\nEpoch 16/250\n233/233 [==============================] - 54s 233ms/step - loss: 1.4970 - accuracy: 0.4847 - val_loss: 1.5150 - val_accuracy: 0.4779\nEpoch 17/250\n233/233 [==============================] - 52s 224ms/step - loss: 1.4622 - accuracy: 0.4945 - val_loss: 1.5746 - val_accuracy: 0.4374\nEpoch 18/250\n233/233 [==============================] - 51s 220ms/step - loss: 1.4229 - accuracy: 0.5056 - val_loss: 1.6251 - val_accuracy: 0.4269\nEpoch 19/250\n233/233 [==============================] - 54s 231ms/step - loss: 1.3883 - accuracy: 0.5138 - val_loss: 1.4976 - val_accuracy: 0.46524 - accuracy: 0 - ETA: 22s - loss: 1.3933 - accurac - ETA: 20s - loss: 1.3927 - accuracy: 0.513 - ETA: 20s - loss: 1.3921 - accura - ETA: 4s - loss: 1.3890 - accura - ETA: 3s\nEpoch 20/250\n233/233 [==============================] - 53s 229ms/step - loss: 1.3545 - accuracy: 0.5204 - val_loss: 1.4435 - val_accuracy: 0.4945\nEpoch 21/250\n233/233 [==============================] - 53s 226ms/step - loss: 1.3260 - accuracy: 0.5278 - val_loss: 1.3890 - val_accuracy: 0.4967\nEpoch 22/250\n233/233 [==============================] - 52s 223ms/step - loss: 1.2997 - accuracy: 0.5358 - val_loss: 1.5169 - val_accuracy: 0.4536\nEpoch 23/250\n233/233 [==============================] - 54s 230ms/step - loss: 1.2713 - accuracy: 0.5472 - val_loss: 1.3843 - val_accuracy: 0.4933\nEpoch 24/250\n233/233 [==============================] - 52s 224ms/step - loss: 1.2527 - accuracy: 0.5502 - val_loss: 1.5209 - val_accuracy: 0.4595\nEpoch 25/250\n233/233 [==============================] - 53s 226ms/step - loss: 1.2372 - accuracy: 0.5555 - val_loss: 1.4133 - val_accuracy: 0.4906\nEpoch 26/250\n233/233 [==============================] - 53s 229ms/step - loss: 1.2178 - accuracy: 0.5584 - val_loss: 1.2513 - val_accuracy: 0.5459ETA\nEpoch 27/250\n233/233 [==============================] - 52s 224ms/step - loss: 1.1961 - accuracy: 0.5709 - val_loss: 1.3028 - val_accuracy: 0.5352\nEpoch 28/250\n233/233 [==============================] - 52s 224ms/step - loss: 1.1838 - accuracy: 0.5736 - val_loss: 1.3017 - val_accuracy: 0.5317\nEpoch 29/250\n233/233 [==============================] - 53s 225ms/step - loss: 1.1664 - accuracy: 0.5724 - val_loss: 1.2201 - val_accuracy: 0.5590\nEpoch 30/250\n233/233 [==============================] - 52s 222ms/step - loss: 1.1485 - accuracy: 0.5838 - val_loss: 1.2392 - val_accuracy: 0.5527\nEpoch 31/250\n233/233 [==============================] - 53s 229ms/step - loss: 1.1400 - accuracy: 0.5873 - val_loss: 1.2104 - val_accuracy: 0.5633\nEpoch 32/250\n233/233 [==============================] - 52s 222ms/step - loss: 1.1289 - accuracy: 0.5869 - val_loss: 1.3091 - val_accuracy: 0.5197\nEpoch 33/250\n233/233 [==============================] - 54s 230ms/step - loss: 1.1139 - accuracy: 0.5907 - val_loss: 1.1726 - val_accuracy: 0.5714\nEpoch 34/250\n233/233 [==============================] - 52s 224ms/step - loss: 1.1057 - accuracy: 0.5967 - val_loss: 1.4290 - val_accuracy: 0.4668\nEpoch 35/250\n233/233 [==============================] - 52s 222ms/step - loss: 1.0928 - accuracy: 0.6042 - val_loss: 1.2089 - val_accuracy: 0.5573\nEpoch 36/250\n233/233 [==============================] - 53s 227ms/step - loss: 1.0847 - accuracy: 0.6048 - val_loss: 1.2332 - val_accuracy: 0.5423\nEpoch 37/250\n233/233 [==============================] - 51s 220ms/step - loss: 1.0689 - accuracy: 0.6102 - val_loss: 1.1830 - val_accuracy: 0.5723\nEpoch 38/250\n233/233 [==============================] - 52s 222ms/step - loss: 1.0569 - accuracy: 0.6142 - val_loss: 1.1864 - val_accuracy: 0.5609\nEpoch 39/250\n233/233 [==============================] - 52s 225ms/step - loss: 1.0515 - accuracy: 0.6143 - val_loss: 1.2536 - val_accuracy: 0.5482\nEpoch 40/250\n233/233 [==============================] - 52s 222ms/step - loss: 1.0473 - accuracy: 0.6187 - val_loss: 1.3205 - val_accuracy: 0.5090:\nEpoch 41/250\n233/233 [==============================] - 53s 226ms/step - loss: 1.0362 - accuracy: 0.6189 - val_loss: 1.1863 - val_accuracy: 0.5702\nEpoch 42/250\n233/233 [==============================] - 53s 228ms/step - loss: 1.0261 - accuracy: 0.6230 - val_loss: 1.1244 - val_accuracy: 0.5963\nEpoch 43/250\n233/233 [==============================] - 51s 220ms/step - loss: 1.0180 - accuracy: 0.6251 - val_loss: 1.1565 - val_accuracy: 0.5863\nEpoch 44/250\n233/233 [==============================] - 53s 230ms/step - loss: 1.0071 - accuracy: 0.6328 - val_loss: 1.0767 - val_accuracy: 0.6085\nEpoch 45/250\n233/233 [==============================] - 51s 220ms/step - loss: 1.0020 - accuracy: 0.6323 - val_loss: 1.1980 - val_accuracy: 0.5551\nEpoch 46/250\n233/233 [==============================] - 51s 219ms/step - loss: 0.9961 - accuracy: 0.6347 - val_loss: 1.1322 - val_accuracy: 0.5880\nEpoch 47/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.9892 - accuracy: 0.6392 - val_loss: 1.1777 - val_accuracy: 0.5714\nEpoch 48/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.9811 - accuracy: 0.6440 - val_loss: 1.1087 - val_accuracy: 0.5959\nEpoch 49/250\n233/233 [==============================] - 53s 227ms/step - loss: 0.9755 - accuracy: 0.6433 - val_loss: 1.1291 - val_accuracy: 0.5854\nEpoch 50/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.9629 - accuracy: 0.6480 - val_loss: 1.0820 - val_accuracy: 0.60469\nEpoch 51/250\n233/233 [==============================] - 51s 220ms/step - loss: 0.9629 - accuracy: 0.6455 - val_loss: 1.1014 - val_accuracy: 0.5975\nEpoch 52/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.9522 - accuracy: 0.6522 - val_loss: 1.0781 - val_accuracy: 0.6066\nEpoch 53/250\n233/233 [==============================] - 51s 218ms/step - loss: 0.9506 - accuracy: 0.6540 - val_loss: 1.1498 - val_accuracy: 0.5890\nEpoch 54/250\n","name":"stdout"},{"output_type":"stream","text":"233/233 [==============================] - 53s 226ms/step - loss: 0.9399 - accuracy: 0.6543 - val_loss: 1.2458 - val_accuracy: 0.5510\nEpoch 55/250\n233/233 [==============================] - 52s 221ms/step - loss: 0.9337 - accuracy: 0.6571 - val_loss: 1.1021 - val_accuracy: 0.6007\nEpoch 56/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.9316 - accuracy: 0.6597 - val_loss: 1.1496 - val_accuracy: 0.5769\nEpoch 57/250\n233/233 [==============================] - 53s 228ms/step - loss: 0.9227 - accuracy: 0.6665 - val_loss: 1.0661 - val_accuracy: 0.6173\nEpoch 58/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.9158 - accuracy: 0.6621 - val_loss: 1.2165 - val_accuracy: 0.5617\nEpoch 59/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.9040 - accuracy: 0.6662 - val_loss: 1.1201 - val_accuracy: 0.6041 0.8959 - accu - ETA: 1s - loss: 0.9032 - accu\nEpoch 60/250\n233/233 [==============================] - 52s 221ms/step - loss: 0.9058 - accuracy: 0.6700 - val_loss: 1.0731 - val_accuracy: 0.6139\nEpoch 61/250\n233/233 [==============================] - 51s 219ms/step - loss: 0.8919 - accuracy: 0.6747 - val_loss: 1.2844 - val_accuracy: 0.5389 los - ETA: 31s - loss: 0.8801 - accuracy: 0. - ETA: 30s - loss: 0.8786 - \nEpoch 62/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.8856 - accuracy: 0.6758 - val_loss: 1.3152 - val_accuracy: 0.5250\nEpoch 63/250\n233/233 [==============================] - 51s 219ms/step - loss: 0.8830 - accuracy: 0.6794 - val_loss: 1.0831 - val_accuracy: 0.6040\nEpoch 64/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.8798 - accuracy: 0.6764 - val_loss: 1.0737 - val_accuracy: 0.6100\nEpoch 65/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.8731 - accuracy: 0.6830 - val_loss: 1.0848 - val_accuracy: 0.6003\nEpoch 66/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.8589 - accuracy: 0.6862 - val_loss: 1.1497 - val_accuracy: 0.6113\nEpoch 67/250\n233/233 [==============================] - 52s 225ms/step - loss: 0.8595 - accuracy: 0.6881 - val_loss: 1.2897 - val_accuracy: 0.5412\nEpoch 68/250\n233/233 [==============================] - 51s 219ms/step - loss: 0.8501 - accuracy: 0.6910 - val_loss: 1.1043 - val_accuracy: 0.6153\nEpoch 69/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.8476 - accuracy: 0.6909 - val_loss: 1.1002 - val_accuracy: 0.6163\nEpoch 70/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.8473 - accuracy: 0.6914 - val_loss: 1.1047 - val_accuracy: 0.5993\nEpoch 71/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.8370 - accuracy: 0.6915 - val_loss: 1.0928 - val_accuracy: 0.6172\nEpoch 72/250\n233/233 [==============================] - 52s 225ms/step - loss: 0.8239 - accuracy: 0.6999 - val_loss: 1.3350 - val_accuracy: 0.5380\nEpoch 73/250\n233/233 [==============================] - 51s 220ms/step - loss: 0.8213 - accuracy: 0.6985 - val_loss: 1.1526 - val_accuracy: 0.5923\nEpoch 74/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.8167 - accuracy: 0.7007 - val_loss: 1.0948 - val_accuracy: 0.6120\nEpoch 75/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.8144 - accuracy: 0.7036 - val_loss: 1.1147 - val_accuracy: 0.6207\nEpoch 76/250\n233/233 [==============================] - 51s 218ms/step - loss: 0.8077 - accuracy: 0.7065 - val_loss: 1.1944 - val_accuracy: 0.5733\nEpoch 77/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.8030 - accuracy: 0.7076 - val_loss: 1.1962 - val_accuracy: 0.5983\nEpoch 78/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.7991 - accuracy: 0.7061 - val_loss: 1.1593 - val_accuracy: 0.5979\nEpoch 79/250\n233/233 [==============================] - 52s 225ms/step - loss: 0.7887 - accuracy: 0.7111 - val_loss: 1.0994 - val_accuracy: 0.6124\nEpoch 80/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.7878 - accuracy: 0.7123 - val_loss: 1.1898 - val_accuracy: 0.5892\nEpoch 81/250\n233/233 [==============================] - 53s 228ms/step - loss: 0.7789 - accuracy: 0.7186 - val_loss: 1.0536 - val_accuracy: 0.6401\nEpoch 82/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.7731 - accuracy: 0.7196 - val_loss: 1.2051 - val_accuracy: 0.5698\nEpoch 83/250\n233/233 [==============================] - 51s 219ms/step - loss: 0.7637 - accuracy: 0.7212 - val_loss: 1.1676 - val_accuracy: 0.6097\nEpoch 84/250\n233/233 [==============================] - 53s 229ms/step - loss: 0.7611 - accuracy: 0.7175 - val_loss: 1.0260 - val_accuracy: 0.6398\nEpoch 85/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.7549 - accuracy: 0.7249 - val_loss: 1.1556 - val_accuracy: 0.6147\nEpoch 86/250\n233/233 [==============================] - 51s 220ms/step - loss: 0.7558 - accuracy: 0.7267 - val_loss: 1.0869 - val_accuracy: 0.6199\nEpoch 87/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.7482 - accuracy: 0.7292 - val_loss: 1.0990 - val_accuracy: 0.6311\nEpoch 88/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.7445 - accuracy: 0.7256 - val_loss: 1.0699 - val_accuracy: 0.6240\nEpoch 89/250\n233/233 [==============================] - 52s 221ms/step - loss: 0.7384 - accuracy: 0.7323 - val_loss: 1.3974 - val_accuracy: 0.5316\nEpoch 90/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.7215 - accuracy: 0.7376 - val_loss: 1.1175 - val_accuracy: 0.6214\nEpoch 91/250\n233/233 [==============================] - 52s 221ms/step - loss: 0.7234 - accuracy: 0.7377 - val_loss: 1.1315 - val_accuracy: 0.6274\nEpoch 92/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.7178 - accuracy: 0.7404 - val_loss: 1.0779 - val_accuracy: 0.6337\nEpoch 93/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.7165 - accuracy: 0.7403 - val_loss: 1.2783 - val_accuracy: 0.5809\nEpoch 94/250\n233/233 [==============================] - 52s 225ms/step - loss: 0.7141 - accuracy: 0.7384 - val_loss: 1.3223 - val_accuracy: 0.5657\nEpoch 95/250\n233/233 [==============================] - 53s 227ms/step - loss: 0.7053 - accuracy: 0.7439 - val_loss: 1.0810 - val_accuracy: 0.6373\nEpoch 96/250\n233/233 [==============================] - 53s 227ms/step - loss: 0.6984 - accuracy: 0.7457 - val_loss: 1.2544 - val_accuracy: 0.5847\nEpoch 97/250\n233/233 [==============================] - 53s 228ms/step - loss: 0.6906 - accuracy: 0.7498 - val_loss: 1.1995 - val_accuracy: 0.6093\nEpoch 98/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.6846 - accuracy: 0.7508 - val_loss: 1.1302 - val_accuracy: 0.6299\nEpoch 99/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.6850 - accuracy: 0.7505 - val_loss: 1.2496 - val_accuracy: 0.6131\nEpoch 100/250\n233/233 [==============================] - 53s 228ms/step - loss: 0.6767 - accuracy: 0.7525 - val_loss: 1.3560 - val_accuracy: 0.5571\nEpoch 101/250\n233/233 [==============================] - 54s 230ms/step - loss: 0.6693 - accuracy: 0.7566 - val_loss: 1.2006 - val_accuracy: 0.6125\nEpoch 102/250\n233/233 [==============================] - 55s 235ms/step - loss: 0.6620 - accuracy: 0.7605 - val_loss: 1.0647 - val_accuracy: 0.6391\nEpoch 103/250\n233/233 [==============================] - 54s 234ms/step - loss: 0.6671 - accuracy: 0.7566 - val_loss: 1.1523 - val_accuracy: 0.6354\nEpoch 104/250\n233/233 [==============================] - 54s 232ms/step - loss: 0.6552 - accuracy: 0.7613 - val_loss: 1.1874 - val_accuracy: 0.6064\nEpoch 105/250\n233/233 [==============================] - 54s 232ms/step - loss: 0.6467 - accuracy: 0.7664 - val_loss: 1.1218 - val_accuracy: 0.6420\nEpoch 106/250\n233/233 [==============================] - 54s 233ms/step - loss: 0.6518 - accuracy: 0.7634 - val_loss: 1.9582 - val_accuracy: 0.4422\nEpoch 107/250\n233/233 [==============================] - 55s 235ms/step - loss: 0.6417 - accuracy: 0.7690 - val_loss: 1.1683 - val_accuracy: 0.6486\nEpoch 108/250\n233/233 [==============================] - 55s 234ms/step - loss: 0.6339 - accuracy: 0.7708 - val_loss: 1.2960 - val_accuracy: 0.5797\n","name":"stdout"},{"output_type":"stream","text":"Epoch 109/250\n233/233 [==============================] - 54s 231ms/step - loss: 0.6316 - accuracy: 0.7719 - val_loss: 1.2213 - val_accuracy: 0.6038\nEpoch 110/250\n233/233 [==============================] - 53s 229ms/step - loss: 0.6241 - accuracy: 0.7737 - val_loss: 1.3065 - val_accuracy: 0.5876\nEpoch 111/250\n233/233 [==============================] - 54s 231ms/step - loss: 0.6145 - accuracy: 0.7793 - val_loss: 1.2494 - val_accuracy: 0.6096\nEpoch 112/250\n233/233 [==============================] - 54s 233ms/step - loss: 0.6164 - accuracy: 0.7777 - val_loss: 1.5120 - val_accuracy: 0.5492\nEpoch 113/250\n233/233 [==============================] - 54s 232ms/step - loss: 0.6114 - accuracy: 0.7776 - val_loss: 1.1804 - val_accuracy: 0.6468\nEpoch 114/250\n233/233 [==============================] - 55s 236ms/step - loss: 0.6019 - accuracy: 0.7825 - val_loss: 1.2313 - val_accuracy: 0.6173\nEpoch 115/250\n233/233 [==============================] - 54s 234ms/step - loss: 0.5977 - accuracy: 0.7829 - val_loss: 1.2777 - val_accuracy: 0.6115\nEpoch 116/250\n233/233 [==============================] - 54s 232ms/step - loss: 0.5969 - accuracy: 0.7825 - val_loss: 1.1847 - val_accuracy: 0.6428\nEpoch 117/250\n233/233 [==============================] - 55s 237ms/step - loss: 0.5958 - accuracy: 0.7827 - val_loss: 1.2605 - val_accuracy: 0.6048\nEpoch 118/250\n233/233 [==============================] - 55s 236ms/step - loss: 0.5850 - accuracy: 0.7850 - val_loss: 1.1897 - val_accuracy: 0.6382\nEpoch 119/250\n233/233 [==============================] - 54s 234ms/step - loss: 0.5843 - accuracy: 0.7859 - val_loss: 1.2763 - val_accuracy: 0.6167\nEpoch 120/250\n233/233 [==============================] - 55s 236ms/step - loss: 0.5811 - accuracy: 0.7867 - val_loss: 1.3010 - val_accuracy: 0.6222\nEpoch 121/250\n233/233 [==============================] - 54s 234ms/step - loss: 0.5749 - accuracy: 0.7924 - val_loss: 1.1862 - val_accuracy: 0.6405\nEpoch 122/250\n233/233 [==============================] - 54s 233ms/step - loss: 0.5548 - accuracy: 0.8002 - val_loss: 1.2037 - val_accuracy: 0.6464\nEpoch 123/250\n233/233 [==============================] - 55s 236ms/step - loss: 0.5619 - accuracy: 0.7938 - val_loss: 1.3673 - val_accuracy: 0.5963\nEpoch 124/250\n233/233 [==============================] - 55s 237ms/step - loss: 0.5618 - accuracy: 0.7969 - val_loss: 1.3165 - val_accuracy: 0.5882\nEpoch 125/250\n233/233 [==============================] - 55s 237ms/step - loss: 0.5483 - accuracy: 0.8041 - val_loss: 1.2478 - val_accuracy: 0.6396\nEpoch 126/250\n233/233 [==============================] - 54s 233ms/step - loss: 0.5461 - accuracy: 0.8043 - val_loss: 1.2607 - val_accuracy: 0.6426\nEpoch 127/250\n233/233 [==============================] - 55s 235ms/step - loss: 0.5474 - accuracy: 0.8018 - val_loss: 1.1926 - val_accuracy: 0.6355\nEpoch 128/250\n233/233 [==============================] - 55s 235ms/step - loss: 0.5285 - accuracy: 0.8091 - val_loss: 1.4255 - val_accuracy: 0.5860\nEpoch 129/250\n233/233 [==============================] - 55s 234ms/step - loss: 0.5330 - accuracy: 0.8084 - val_loss: 1.3450 - val_accuracy: 0.6151\nEpoch 130/250\n233/233 [==============================] - 55s 236ms/step - loss: 0.5298 - accuracy: 0.8078 - val_loss: 1.3383 - val_accuracy: 0.6123\nEpoch 131/250\n233/233 [==============================] - 55s 236ms/step - loss: 0.5380 - accuracy: 0.8035 - val_loss: 1.3921 - val_accuracy: 0.6090\nEpoch 132/250\n233/233 [==============================] - 54s 231ms/step - loss: 0.5223 - accuracy: 0.8124 - val_loss: 1.3539 - val_accuracy: 0.6302\nEpoch 133/250\n233/233 [==============================] - 54s 232ms/step - loss: 0.5124 - accuracy: 0.8166 - val_loss: 1.5452 - val_accuracy: 0.5751\nEpoch 134/250\n233/233 [==============================] - 55s 235ms/step - loss: 0.5073 - accuracy: 0.8181 - val_loss: 1.4219 - val_accuracy: 0.6185\nEpoch 135/250\n233/233 [==============================] - 54s 234ms/step - loss: 0.5070 - accuracy: 0.8173 - val_loss: 1.3569 - val_accuracy: 0.6124\nEpoch 136/250\n233/233 [==============================] - 55s 238ms/step - loss: 0.5021 - accuracy: 0.8208 - val_loss: 1.4087 - val_accuracy: 0.6044\nEpoch 137/250\n233/233 [==============================] - 55s 236ms/step - loss: 0.4970 - accuracy: 0.8211 - val_loss: 1.3750 - val_accuracy: 0.6123\nEpoch 138/250\n233/233 [==============================] - 55s 234ms/step - loss: 0.4969 - accuracy: 0.8227 - val_loss: 1.3404 - val_accuracy: 0.6097\nEpoch 139/250\n233/233 [==============================] - 55s 234ms/step - loss: 0.4778 - accuracy: 0.8281 - val_loss: 1.4276 - val_accuracy: 0.6215\nEpoch 140/250\n233/233 [==============================] - 55s 236ms/step - loss: 0.4901 - accuracy: 0.8257 - val_loss: 1.4783 - val_accuracy: 0.5902\nEpoch 141/250\n233/233 [==============================] - 56s 239ms/step - loss: 0.4840 - accuracy: 0.8257 - val_loss: 1.2058 - val_accuracy: 0.6591\nEpoch 142/250\n233/233 [==============================] - 55s 235ms/step - loss: 0.4791 - accuracy: 0.8280 - val_loss: 1.2855 - val_accuracy: 0.6464\nEpoch 143/250\n233/233 [==============================] - 55s 237ms/step - loss: 0.4705 - accuracy: 0.8330 - val_loss: 1.2340 - val_accuracy: 0.6458\nEpoch 144/250\n233/233 [==============================] - 55s 235ms/step - loss: 0.4643 - accuracy: 0.8347 - val_loss: 1.3951 - val_accuracy: 0.6139\nEpoch 145/250\n233/233 [==============================] - 55s 234ms/step - loss: 0.4576 - accuracy: 0.8354 - val_loss: 1.3897 - val_accuracy: 0.5959\nEpoch 146/250\n233/233 [==============================] - 54s 232ms/step - loss: 0.4548 - accuracy: 0.8345 - val_loss: 1.4638 - val_accuracy: 0.6179\nEpoch 147/250\n233/233 [==============================] - 55s 237ms/step - loss: 0.4522 - accuracy: 0.8383 - val_loss: 1.3726 - val_accuracy: 0.6243\nEpoch 148/250\n233/233 [==============================] - 55s 235ms/step - loss: 0.4490 - accuracy: 0.8406 - val_loss: 1.3332 - val_accuracy: 0.6452\nEpoch 149/250\n233/233 [==============================] - 54s 230ms/step - loss: 0.4446 - accuracy: 0.8406 - val_loss: 1.2692 - val_accuracy: 0.6417\nEpoch 150/250\n233/233 [==============================] - 54s 231ms/step - loss: 0.4386 - accuracy: 0.8422 - val_loss: 1.3829 - val_accuracy: 0.6183\nEpoch 151/250\n233/233 [==============================] - 53s 227ms/step - loss: 0.4332 - accuracy: 0.8435 - val_loss: 1.4318 - val_accuracy: 0.6511\nEpoch 152/250\n233/233 [==============================] - 53s 227ms/step - loss: 0.4349 - accuracy: 0.8435 - val_loss: 1.3832 - val_accuracy: 0.6442\nEpoch 153/250\n233/233 [==============================] - 53s 229ms/step - loss: 0.4366 - accuracy: 0.8421 - val_loss: 1.3660 - val_accuracy: 0.6456\nEpoch 154/250\n233/233 [==============================] - 51s 220ms/step - loss: 0.4195 - accuracy: 0.8489 - val_loss: 1.4524 - val_accuracy: 0.6036\nEpoch 155/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.4243 - accuracy: 0.8501 - val_loss: 1.2800 - val_accuracy: 0.6590\nEpoch 156/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.4249 - accuracy: 0.8484 - val_loss: 1.3374 - val_accuracy: 0.6436\nEpoch 157/250\n233/233 [==============================] - 52s 225ms/step - loss: 0.4125 - accuracy: 0.8513 - val_loss: 1.4379 - val_accuracy: 0.6284\nEpoch 158/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.4114 - accuracy: 0.8534 - val_loss: 1.3992 - val_accuracy: 0.6441\nEpoch 159/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.4052 - accuracy: 0.8551 - val_loss: 1.3396 - val_accuracy: 0.6654\nEpoch 160/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.4051 - accuracy: 0.8566 - val_loss: 1.7465 - val_accuracy: 0.5644\nEpoch 161/250\n233/233 [==============================] - 53s 227ms/step - loss: 0.3993 - accuracy: 0.8572 - val_loss: 1.5419 - val_accuracy: 0.6248\nEpoch 162/250\n233/233 [==============================] - 52s 221ms/step - loss: 0.3998 - accuracy: 0.8579 - val_loss: 1.4309 - val_accuracy: 0.6268\nEpoch 163/250\n233/233 [==============================] - 52s 221ms/step - loss: 0.3930 - accuracy: 0.8605 - val_loss: 1.4255 - val_accuracy: 0.6433\nEpoch 164/250\n","name":"stdout"},{"output_type":"stream","text":"233/233 [==============================] - 52s 224ms/step - loss: 0.3910 - accuracy: 0.8576 - val_loss: 1.4139 - val_accuracy: 0.6398\nEpoch 165/250\n233/233 [==============================] - 53s 227ms/step - loss: 0.3918 - accuracy: 0.8616 - val_loss: 1.3623 - val_accuracy: 0.6315\nEpoch 166/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.3830 - accuracy: 0.8669 - val_loss: 1.3712 - val_accuracy: 0.6440\nEpoch 167/250\n233/233 [==============================] - 51s 221ms/step - loss: 0.3811 - accuracy: 0.8660 - val_loss: 1.5805 - val_accuracy: 0.6169\nEpoch 168/250\n233/233 [==============================] - 52s 225ms/step - loss: 0.3814 - accuracy: 0.8659 - val_loss: 1.7447 - val_accuracy: 0.5747\nEpoch 169/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.3705 - accuracy: 0.8708 - val_loss: 1.4585 - val_accuracy: 0.6234 0.3659 - acc - ETA: 13s - loss: \nEpoch 170/250\n233/233 [==============================] - 52s 221ms/step - loss: 0.3714 - accuracy: 0.8679 - val_loss: 1.3806 - val_accuracy: 0.6424\nEpoch 171/250\n233/233 [==============================] - 51s 219ms/step - loss: 0.3660 - accuracy: 0.8690 - val_loss: 1.4128 - val_accuracy: 0.6491\nEpoch 172/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.3624 - accuracy: 0.8702 - val_loss: 1.4364 - val_accuracy: 0.6397\nEpoch 173/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.3617 - accuracy: 0.8710 - val_loss: 1.8894 - val_accuracy: 0.5629\nEpoch 174/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.3595 - accuracy: 0.8727 - val_loss: 1.4895 - val_accuracy: 0.6295\nEpoch 175/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.3523 - accuracy: 0.8737 - val_loss: 1.5351 - val_accuracy: 0.6383\nEpoch 176/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.3420 - accuracy: 0.8773 - val_loss: 1.5051 - val_accuracy: 0.6367\nEpoch 177/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.3485 - accuracy: 0.8759 - val_loss: 1.3897 - val_accuracy: 0.6466\nEpoch 178/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.3415 - accuracy: 0.8784 - val_loss: 1.6437 - val_accuracy: 0.6093\nEpoch 179/250\n233/233 [==============================] - 51s 221ms/step - loss: 0.3408 - accuracy: 0.8782 - val_loss: 1.4820 - val_accuracy: 0.6404\nEpoch 180/250\n233/233 [==============================] - 53s 229ms/step - loss: 0.3326 - accuracy: 0.8838 - val_loss: 1.6915 - val_accuracy: 0.6082\nEpoch 181/250\n233/233 [==============================] - 53s 228ms/step - loss: 0.3285 - accuracy: 0.8818 - val_loss: 1.5679 - val_accuracy: 0.6244\nEpoch 182/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.3339 - accuracy: 0.8807 - val_loss: 1.5067 - val_accuracy: 0.6465\nEpoch 183/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.3211 - accuracy: 0.8880 - val_loss: 1.4862 - val_accuracy: 0.6440\nEpoch 184/250\n233/233 [==============================] - 51s 220ms/step - loss: 0.3258 - accuracy: 0.8841 - val_loss: 1.8432 - val_accuracy: 0.6020\nEpoch 185/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.3267 - accuracy: 0.8859 - val_loss: 1.4196 - val_accuracy: 0.6458\nEpoch 186/250\n233/233 [==============================] - 51s 220ms/step - loss: 0.3204 - accuracy: 0.8875 - val_loss: 1.4566 - val_accuracy: 0.6519\nEpoch 187/250\n233/233 [==============================] - 51s 220ms/step - loss: 0.3151 - accuracy: 0.8899 - val_loss: 1.5329 - val_accuracy: 0.6302\nEpoch 188/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.3185 - accuracy: 0.8895 - val_loss: 1.5032 - val_accuracy: 0.6493\nEpoch 189/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.3105 - accuracy: 0.8914 - val_loss: 1.7178 - val_accuracy: 0.6022\nEpoch 190/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.3108 - accuracy: 0.8899 - val_loss: 1.4785 - val_accuracy: 0.6501\nEpoch 191/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.3033 - accuracy: 0.8930 - val_loss: 1.3947 - val_accuracy: 0.6612\nEpoch 192/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.3036 - accuracy: 0.8925 - val_loss: 1.4910 - val_accuracy: 0.6426\nEpoch 193/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.2986 - accuracy: 0.8962 - val_loss: 1.6305 - val_accuracy: 0.6428\nEpoch 194/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.2959 - accuracy: 0.8962 - val_loss: 1.7309 - val_accuracy: 0.6406\nEpoch 195/250\n233/233 [==============================] - 51s 221ms/step - loss: 0.2992 - accuracy: 0.8958 - val_loss: 1.5720 - val_accuracy: 0.6354\nEpoch 196/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.2971 - accuracy: 0.8953 - val_loss: 1.5003 - val_accuracy: 0.6382\nEpoch 197/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.2951 - accuracy: 0.8959 - val_loss: 1.5143 - val_accuracy: 0.6366\nEpoch 198/250\n233/233 [==============================] - 52s 221ms/step - loss: 0.2878 - accuracy: 0.8968 - val_loss: 1.5285 - val_accuracy: 0.6318\nEpoch 199/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.2920 - accuracy: 0.8972 - val_loss: 1.4844 - val_accuracy: 0.6461\nEpoch 200/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.2828 - accuracy: 0.8999 - val_loss: 1.5370 - val_accuracy: 0.6535\nEpoch 201/250\n233/233 [==============================] - 51s 220ms/step - loss: 0.2870 - accuracy: 0.8980 - val_loss: 1.5534 - val_accuracy: 0.6579s - lo - ETA: 6s - loss: 0.2\nEpoch 202/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.2786 - accuracy: 0.9018 - val_loss: 1.5326 - val_accuracy: 0.6410\nEpoch 203/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.2782 - accuracy: 0.9025 - val_loss: 1.5015 - val_accuracy: 0.6615\nEpoch 204/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.2768 - accuracy: 0.9040 - val_loss: 1.7551 - val_accuracy: 0.5941\nEpoch 205/250\n233/233 [==============================] - 52s 225ms/step - loss: 0.2732 - accuracy: 0.9065 - val_loss: 1.5618 - val_accuracy: 0.6540\nEpoch 206/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.2737 - accuracy: 0.9035 - val_loss: 1.4840 - val_accuracy: 0.6559\nEpoch 207/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.2648 - accuracy: 0.9089 - val_loss: 1.5686 - val_accuracy: 0.6564\nEpoch 208/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.2741 - accuracy: 0.9041 - val_loss: 1.5189 - val_accuracy: 0.6524\nEpoch 209/250\n233/233 [==============================] - 52s 221ms/step - loss: 0.2691 - accuracy: 0.9056 - val_loss: 1.4380 - val_accuracy: 0.6622\nEpoch 210/250\n233/233 [==============================] - 52s 221ms/step - loss: 0.2613 - accuracy: 0.9072 - val_loss: 1.6484 - val_accuracy: 0.6370\nEpoch 211/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.2634 - accuracy: 0.9084 - val_loss: 1.5982 - val_accuracy: 0.6302\nEpoch 212/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.2587 - accuracy: 0.9108 - val_loss: 1.5654 - val_accuracy: 0.6318\nEpoch 213/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.2552 - accuracy: 0.9117 - val_loss: 1.5249 - val_accuracy: 0.6557\nEpoch 214/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.2525 - accuracy: 0.9118 - val_loss: 1.5813 - val_accuracy: 0.6543\nEpoch 215/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.2522 - accuracy: 0.9126 - val_loss: 1.5485 - val_accuracy: 0.6606\nEpoch 216/250\n233/233 [==============================] - 51s 219ms/step - loss: 0.2565 - accuracy: 0.9108 - val_loss: 1.5532 - val_accuracy: 0.6553\nEpoch 217/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.2544 - accuracy: 0.9122 - val_loss: 1.5589 - val_accuracy: 0.6556\nEpoch 218/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.2451 - accuracy: 0.9154 - val_loss: 1.8630 - val_accuracy: 0.5933\nEpoch 219/250\n","name":"stdout"},{"output_type":"stream","text":"233/233 [==============================] - 52s 225ms/step - loss: 0.2487 - accuracy: 0.9132 - val_loss: 1.7768 - val_accuracy: 0.6408\nEpoch 220/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.2441 - accuracy: 0.9141 - val_loss: 1.5711 - val_accuracy: 0.6547\nEpoch 221/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.2427 - accuracy: 0.9149 - val_loss: 1.5924 - val_accuracy: 0.6547\nEpoch 222/250\n233/233 [==============================] - 53s 227ms/step - loss: 0.2397 - accuracy: 0.9158 - val_loss: 1.5661 - val_accuracy: 0.6448\nEpoch 223/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.2351 - accuracy: 0.9186 - val_loss: 1.8242 - val_accuracy: 0.6037\nEpoch 224/250\n233/233 [==============================] - 52s 222ms/step - loss: 0.2369 - accuracy: 0.9185 - val_loss: 1.5705 - val_accuracy: 0.6614\nEpoch 225/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.2321 - accuracy: 0.9211 - val_loss: 1.6530 - val_accuracy: 0.6331\nEpoch 226/250\n233/233 [==============================] - 51s 219ms/step - loss: 0.2320 - accuracy: 0.9196 - val_loss: 1.7891 - val_accuracy: 0.6318\nEpoch 227/250\n233/233 [==============================] - 53s 225ms/step - loss: 0.2313 - accuracy: 0.9193 - val_loss: 1.6721 - val_accuracy: 0.6537loss: - ETA: 1s - loss: 0.230\nEpoch 228/250\n233/233 [==============================] - 52s 224ms/step - loss: 0.2336 - accuracy: 0.9191 - val_loss: 1.5352 - val_accuracy: 0.6497\nEpoch 229/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.2299 - accuracy: 0.9203 - val_loss: 1.6767 - val_accuracy: 0.6315\nEpoch 230/250\n233/233 [==============================] - 53s 229ms/step - loss: 0.2285 - accuracy: 0.9234 - val_loss: 1.5629 - val_accuracy: 0.6615\nEpoch 231/250\n233/233 [==============================] - 53s 225ms/step - loss: 0.2313 - accuracy: 0.9210 - val_loss: 1.5764 - val_accuracy: 0.6420\nEpoch 232/250\n233/233 [==============================] - 52s 223ms/step - loss: 0.2295 - accuracy: 0.9210 - val_loss: 1.7275 - val_accuracy: 0.6301\nEpoch 233/250\n233/233 [==============================] - 53s 226ms/step - loss: 0.2182 - accuracy: 0.9242 - val_loss: 1.6668 - val_accuracy: 0.6482\nEpoch 234/250\n233/233 [==============================] - 53s 227ms/step - loss: 0.2178 - accuracy: 0.9248 - val_loss: 1.7261 - val_accuracy: 0.6485\nEpoch 235/250\n233/233 [==============================] - 54s 232ms/step - loss: 0.2208 - accuracy: 0.9251 - val_loss: 1.6099 - val_accuracy: 0.6668\nEpoch 236/250\n233/233 [==============================] - 53s 230ms/step - loss: 0.2202 - accuracy: 0.9232 - val_loss: 1.6617 - val_accuracy: 0.6660\nEpoch 237/250\n233/233 [==============================] - 54s 233ms/step - loss: 0.2218 - accuracy: 0.9239 - val_loss: 1.6831 - val_accuracy: 0.6486\nEpoch 238/250\n233/233 [==============================] - 54s 234ms/step - loss: 0.2187 - accuracy: 0.9245 - val_loss: 1.8466 - val_accuracy: 0.6214\nEpoch 239/250\n233/233 [==============================] - 54s 231ms/step - loss: 0.2183 - accuracy: 0.9237 - val_loss: 1.6499 - val_accuracy: 0.6523\nEpoch 240/250\n233/233 [==============================] - 55s 234ms/step - loss: 0.2116 - accuracy: 0.9261 - val_loss: 1.8674 - val_accuracy: 0.6216\nEpoch 241/250\n233/233 [==============================] - 55s 235ms/step - loss: 0.2132 - accuracy: 0.9274 - val_loss: 1.5655 - val_accuracy: 0.6640\nEpoch 242/250\n233/233 [==============================] - 54s 233ms/step - loss: 0.2090 - accuracy: 0.9277 - val_loss: 1.6158 - val_accuracy: 0.6640\nEpoch 243/250\n233/233 [==============================] - 54s 231ms/step - loss: 0.2122 - accuracy: 0.9274 - val_loss: 1.7101 - val_accuracy: 0.6547\nEpoch 244/250\n233/233 [==============================] - 53s 230ms/step - loss: 0.2070 - accuracy: 0.9277 - val_loss: 1.6579 - val_accuracy: 0.6476\nEpoch 245/250\n233/233 [==============================] - 55s 234ms/step - loss: 0.2070 - accuracy: 0.9269 - val_loss: 1.6384 - val_accuracy: 0.6567\nEpoch 246/250\n233/233 [==============================] - 54s 231ms/step - loss: 0.2055 - accuracy: 0.9302 - val_loss: 1.6051 - val_accuracy: 0.6577\nEpoch 247/250\n233/233 [==============================] - 55s 238ms/step - loss: 0.2080 - accuracy: 0.9276 - val_loss: 1.7890 - val_accuracy: 0.6295\nEpoch 248/250\n233/233 [==============================] - 54s 233ms/step - loss: 0.2010 - accuracy: 0.9311 - val_loss: 1.7901 - val_accuracy: 0.6272\nEpoch 249/250\n233/233 [==============================] - 54s 233ms/step - loss: 0.2028 - accuracy: 0.9289 - val_loss: 1.7508 - val_accuracy: 0.6449\nEpoch 250/250\n233/233 [==============================] - 55s 235ms/step - loss: 0.2001 - accuracy: 0.9296 - val_loss: 1.7144 - val_accuracy: 0.6591\n[INFO] evaluating network...\n[[1.94753895e-07 1.53630690e-08 1.45702055e-04 ... 1.42202441e-06\n  3.36280443e-08 9.99816358e-01]\n [5.13114333e-02 1.95953035e-04 8.95726159e-02 ... 3.14062804e-01\n  5.40550172e-01 7.47328100e-04]\n [5.94113052e-01 2.87633797e-04 2.68804785e-02 ... 2.20997229e-01\n  1.44338369e-01 8.17062391e-04]\n ...\n [2.62342095e-01 2.05163713e-04 6.16949499e-02 ... 5.19945577e-04\n  2.96020810e-03 6.60091685e-03]\n [8.33810687e-01 1.37872085e-01 1.31387785e-02 ... 4.26039565e-04\n  1.40069267e-02 3.84534578e-05]\n [1.33937850e-04 2.98574423e-06 9.95487869e-01 ... 2.16968547e-05\n  2.51274114e-03 1.79451832e-03]]\n[6 5 0 ... 3 0 2]\n              precision    recall  f1-score   support\n\n       Angry       0.57      0.57      0.57      1035\n     Disgust       0.66      0.66      0.66       160\n        Fear       0.47      0.51      0.49      1055\n       Happy       0.85      0.84      0.85      1855\n     Neutral       0.57      0.64      0.60      1255\n         Sad       0.52      0.47      0.49      1248\n    Surprise       0.78      0.69      0.73       866\n\n    accuracy                           0.64      7474\n   macro avg       0.63      0.63      0.63      7474\nweighted avg       0.64      0.64      0.64      7474\n\n[INFO] saving mask detector model...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = H.history\nprint(history_dict.keys())\n\n# plot the training loss and accuracy\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\n# plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n# plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_accuracy\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.title(\"Training and validation Accuracy for 55 layer residual network\")\nplt.xlabel(f\"Epoch {EPOCHS}\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(\"/kaggle/working/plot\")","execution_count":47,"outputs":[{"output_type":"stream","text":"dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}