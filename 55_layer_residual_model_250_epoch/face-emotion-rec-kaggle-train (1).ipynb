{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"! pip install imutils","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting imutils\n  Downloading imutils-0.5.4.tar.gz (17 kB)\nBuilding wheels for collected packages: imutils\n  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25858 sha256=b363c098fd7d1f77747ce20b068ed4fe1e9ea7f2e23a57a3bc05946844c0bc60\n  Stored in directory: /root/.cache/pip/wheels/86/d7/0a/4923351ed1cec5d5e24c1eaf8905567b02a0343b24aa873df2\nSuccessfully built imutils\nInstalling collected packages: imutils\nSuccessfully installed imutils-0.5.4\n\u001b[33mWARNING: You are using pip version 20.3.1; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\n# from tensorflow.keras.models import load_model\n# from keras.layers import LeakyReLU\nfrom sklearn.preprocessing import LabelBinarizer,LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport os\nimport cv2","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"[INFO] loading images...\")\nimagePaths = list(paths.list_images(\"../input/main-dataset/main dataset\"))\n# print(imagePaths)\ndata = []\nlabels = []","execution_count":null,"outputs":[{"output_type":"stream","text":"[INFO] loading images...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for imagePath in imagePaths:\n    # extract the class label from the filename\n    label = imagePath.split(os.path.sep)[-2]\n\n\t# load the input image (224x224) and preprocess it\n    image = load_img(imagePath, target_size=(48, 48))\n    image = img_to_array(image)\n    if label=='angry':\n        l = [1,0,0,0,0,0]\n    elif label=='fear':\n        l = [0,1,0,0,0,0]\n    elif label=='happy':\n        l = [0,0,1,0,0,0]\n    elif label=='neutral':\n        l = [0,0,0,1,0,0]\n    elif label=='sad':\n        l = [0,0,0,0,1,0]\n    elif label=='surprise':\n        l = [0,0,0,0,0,1]\n        \n# \timage = preprocess_input(image)\n\t# update the data and labels lists, respectively\n\t# count+=1/\n\t# if count%10==0:\n    data.append(image)\n    labels.append(l)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.array(data, dtype=\"float32\")\nprint(type(data))\nlabels = np.array(labels)\nprint(\"labels\",labels.shape)\n# perform one-hot encoding on the labels\n# lb = LabelEncoder()\n# labels = lb.fit_transform(labels1)\n# labels = to_categorical(labels)\nprint(labels,type(labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape,labels.shape)\n(trainX, testX, trainY, testY) = train_test_split(data, labels,\n\ttest_size=0.2, stratify=labels, random_state=42)\nprint(trainX.shape, testX.shape, trainY.shape, testY.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aug = ImageDataGenerator(\n\trotation_range=20,\n\tzoom_range=0.15,\n\twidth_shift_range=0.2,\n\theight_shift_range=0.2,\n\tshear_range=0.15,\n\thorizontal_flip=True,\n\tfill_mode=\"nearest\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import InputLayer,Conv2D,MaxPool2D,BatchNormalization,ZeroPadding2D,ReLU,SeparableConv2D\nfrom keras import regularizers\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.models import load_model\n\nimport numpy as np\nfrom keras import layers\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.models import Model, load_model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nimport pydot\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\n# from resnets_utils import *\nfrom keras.initializers import glorot_uniform\nimport scipy.misc\nfrom matplotlib.pyplot import imshow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def identity_block(X, f, filters, stage, block):\n    \n    # Defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value\n    X_shortcut = X\n    \n    # First component of main path\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n    \n    # Second component of main path\n    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n    X = Activation('relu')(X)\n\n    # Third component of main path \n    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n\n    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convolutional_block(X, f, filters, stage, block, s=2):\n\n    # Defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    # Retrieve Filters\n    F1, F2, F3 = filters\n\n    # Save the input value\n    X_shortcut = X\n\n    ##### MAIN PATH #####\n    # First component of main path \n    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n\n    # Second component of main path\n    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n    X = Activation('relu')(X)\n\n    # Third component of main path\n    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n\n    ##### SHORTCUT PATH #### \n    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n\n    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ResNet50(input_shape = (72, 72, 3), classes = 7):   \n    \n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n\n    \n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n    # Stage 2\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n    \n    X = Dropout(0.15)(X)\n\n    # Stage 3\n    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n    \n    X = Dropout(0.15)(X)\n\n    # Stage 4\n    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n    \n    X = Dropout(0.15)(X)\n\n    \n\n    # Stage 5\n    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n\n    # AVGPOOL\n    X = AveragePooling2D(pool_size=(2,2), padding='same')(X)\n\n    # Output layer\n    X = Flatten()(X)\n    X = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(X)\n    X = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(X)\n    X = Dropout(0.15)(X)\n    X = Dense(156, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(X)\n    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n    \n    \n    # Create model\n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INIT_LR = 1e-4\nEPOCHS = 250\nBS = 128\n\n\n# baseModel = Sequential()\n# # baseModel.add(Dense(12, activation='relu', input_tensor = Input(shape=(48,48,3))))\n# baseModel.add(InputLayer(input_shape = (48,48,3)))\n# # baseModel = tf.keras.applications.Xception(input_shape=(72,72,3),include_top=False,weights='imagenet')\n# headModel = baseModel.output\n# # headModel = ZeroPadding2D()(headModel)\n# headModel = Conv2D(72,3,activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = Conv2D(72,3,activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = SeparableConv2D(72,3,activation=\"relu\",use_bias=True,strides=(2, 2),kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = Conv2D(144,3,activation=\"relu\",padding = \"same\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = SeparableConv2D(144,3,activation=\"relu\",use_bias=True,strides=(1, 1),kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = MaxPool2D(pool_size = (2,2))(headModel)\n# headModel = Dropout(0.15)(headModel)\n\n# headModel = Conv2D(288,3,activation=\"relu\",padding = \"same\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = Conv2D(288,3,activation=\"relu\",padding = \"same\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = BatchNormalization()(headModel)\n# headModel = SeparableConv2D(288,3,activation=\"relu\",use_bias=True,strides=(2, 2),kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = BatchNormalization()(headModel)\n# # headModel = MaxPool2D(pool_size = (2,2))(headModel)\n# headModel = Dropout(0.15)(headModel)\n\n\n\n\n# headModel = Conv2D(288,3,activation=\"relu\",padding = \"same\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = Conv2D(288,3,activation=\"relu\",padding = \"same\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = Conv2D(288,3,activation=\"relu\",padding = \"same\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = MaxPool2D(pool_size = (2,2))(headModel)\n\n\n# # construct the head of the model that will be placed on top of the\n# # the base model\n# # headModel = baseModel.output\n# # headModel = AveragePooling2D(pool_size=(1, 1))(headModel)\n# headModel = Flatten(name=\"flatten\")(headModel)\n# # headModel = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# # headModel = Dropout(0.15)(headModel)\n# headModel = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = Dense(512, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# headModel = Dropout(0.15)(headModel)\n# headModel = Dense(156, activation=\"relu\",kernel_regularizer=regularizers.l2(0.001))(headModel)\n# # headModel = Dropout(0.1)(headModel)\n# # headModel = Dense(256, activation=\"relu\",kernel_initializer = 'he_normal')(headModel)\n# # headModel = Dense(1024, activation=\"relu\")(headModel)\n# # headModel = Dense(256, activation=\"relu\")(headModel)\n# headModel = Dense(6, activation=\"softmax\")(headModel)\n\n# # place the head FC model on top of the base model (this will become\n# # the actual model we will train)\n# model = Model(inputs=baseModel.input, outputs=headModel)\n\n\n# checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n#                              monitor='val_loss',\n#                              mode='min',\n#                              save_best_only=True,\n#                              verbose=0)\n\n# # earlystop = EarlyStopping(monitor='val_loss',\n# #                           min_delta=0,\n# #                           patience=3,\n# #                           verbose=1,\n# #                           restore_best_weights=True\n# #                           )\n\n# # reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n# #                               factor=0.2,\n# #                               patience=5,\n# #                               verbose=1,\n# #                               min_delta=0.0001)\n\n# callbacks = [checkpoint]\n\n# # for layer in baseModel.layers:\n# # \tlayer.trainable = False\n\n# print(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ResNet50(input_shape = (48, 48, 3), classes = 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n                             monitor='val_loss',\n                             mode='min',\n                             save_best_only=True,\n                             verbose=0)\n\ncallbacks = [checkpoint]\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for layer in baseModel.layers:\n# \tlayer.trainable = False\n\n# compile our model\nprint(\"[INFO] compiling model...\")\nopt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the head of the network\nprint(\"[INFO] training head...\")\nH = model.fit(\n\taug.flow(trainX, trainY, batch_size=BS),\n\tsteps_per_epoch=len(trainX) // BS,\n\tvalidation_data=(testX, testY),\n    callbacks=callbacks,\n\tvalidation_steps=len(testX) // BS,\n\tepochs=EPOCHS)\n\nmodel = load_model(\"EmotionDetectionModel.h5\")\n\n\n# make predictions on the testing set\nprint(\"[INFO] evaluating network...\")\npredIdxs = model.predict(testX, batch_size=BS)\nprint(predIdxs)\n# for each image in the testing set we need to find the index of the\n# label with corresponding largest predicted probability\npredIdxs = np.argmax(predIdxs, axis=1)\nprint(predIdxs)\n\n# show a nicely formatted classification report\n# print(classification_report(testY.argmax(axis=1), predIdxs,\n# \ttarget_names=lb.classes_))\n\n# serialize the model to disk\nprint(\"[INFO] saving mask detector model...\")\nmodel.save(\"/kaggle/working/model.h5\", save_format=\"h5\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = H.history\nprint(history_dict.keys())\n\n# plot the training loss and accuracy\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_accuracy\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.title(\"Training & val loss & Accuracy for 25 layer sep-conv network\")\nplt.xlabel(f\"Epoch {EPOCHS}\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(\"/kaggle/working/loss_accuracy_plot\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n# plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_accuracy\")\n# plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.title(\"Training & val loss for 55 layer residual conv network\")\nplt.xlabel(f\"Epoch {EPOCHS}\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(\"/kaggle/working/loss_plot\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\n# plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n# plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_accuracy\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.title(\"Training & val loss & Accuracy for 25 layer sep-conv network\")\nplt.xlabel(f\"Epoch {EPOCHS}\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(\"/kaggle/working/accuracy_plot\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}